\documentclass[letterpaper,twoside,11pt]{article}
\usepackage{a4wide,graphicx,fancyhdr,clrscode,tabularx,amsmath,amssymb,amsfonts,color,enumitem, bm, bbm, array, textcomp, subcaption, color, listings, chemformula, tcolorbox, setspace, xcolor, hyperref}
\usepackage{amsthm}		%theorem style 
%\usepackage{geometry}  %Adjust margins if needed. 
%\geometry{margin=1.25in} 
%\usepackage{mathptmx}      %SET MATH TYPE FONT TO TIMES NEW ROMAN
%These lines make the theorem NAME BOLD
\newtheoremstyle{mystyle}%                % Name
  {}%                                     % Space above
  {}%                                     % Space below
  {\itshape}%                                     % Body font
  {}%                                     % Indent amount
  {\bfseries}%                            % Theorem head font
  {.}%                                    % Punctuation after theorem head
  { }%                                    % Space after theorem head, ' ', or \newline
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}%                                     % Theorem head spec (can be left empty, meaning `normal')
\theoremstyle{mystyle}
\newtheorem{theorem}{Theorem}[section]
%end of making theorem name bold. 
\newtheorem*{thm}{Theorem}		%Theorem no number. 
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{lemm}{Lemma}
\newtheorem{prop}{Proposition}[section]
\newtheorem*{propp}{Proposition}
\newtheorem*{ex}{Example}
\newtheorem*{example}{Example}
\newtheorem{notee}{Note}[section]
\newtheorem*{note}{Note}
\usepackage[super]{nth}
\usepackage[makeroom]{cancel}

%----------------------- Macros and Definitions --------------------------
\setlength{\fboxsep}{2.5\fboxsep}
%Sets boxlength size

\setlength\headheight{15pt}
\addtolength\topmargin{-25pt}
\addtolength\footskip{0pt}

\fancypagestyle{plain}{%
\fancyhf{}
\fancyhead[LO,RE]{\sffamily UT Austin}
\fancyhead[RO,LE]{\sffamily CSE 386C}
\fancyfoot[LO,RE]{\sffamily Oden Institute}
\fancyfoot[RO,LE]{\sffamily\bfseries\thepage} 
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[RO,LE]{\sffamily CSE 386C}
\fancyhead[LO,RE]{\sffamily UT Austin}
\fancyfoot[LO,RE]{\sffamily Oden Institute}
\fancyfoot[RO,LE]{\sffamily\bfseries\thepage}
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{0pt}
\newcommand{\R}{{\mathbb R}}
\newcommand{\N}{{\mathbb N}}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\Q}{{\mathbb Q}}
\newcommand{\C}{{\mathbb C}}
\newcommand{\SL}{{\mathcal{L}}}
\newcommand{\DD}{\mathcal D}
\newcommand{\ScS}{\mathcal S}
\newcommand{\Om}{\Omega}
\DeclareMathOperator*{\slim}{s-lim}
\newcommand{\cg}{\color{gray}}
\newcommand{\cbk}{\color{black}}
\newcommand{\cred}{\color{red}}
\newcommand{\cblu}{\color{blue}}
\usepackage{libertine}            %% For fancy font
\begin{document}
%\fontfamily{ptm}\selectfont     %% TO SELECT THE FONT
\title{\vspace{-2\baselineskip} 
Some Notes and Theorems
}
\author{Jonathan Zhang}
\date{}
\maketitle


\section{Preliminaries}
\subsection{Topology and Metrics}
Topology - notions of the following: 
\begin{itemize}
\item A set $G$ is open iff for every $x\in G$, there exists an $r>0$ such that $y\in G$ whenever $d(x, y) < r$. 
\item A set $F$ is closed iff for every sequence $x_n\in F$ converging to $x$, that $x\in F$. 
\end{itemize}
Other notions: 
\begin{itemize}
\item Compact set
\item Open cover
\item Limits
\item Continuous functions: a function $f$ is continuous iff the inverse image of a closed set is closed. 
\item Continuity at a point
\item Product topologies
\end{itemize}

\subsection*{Metric Spaces}
\begin{definition}
A function $d: S\times S \to \R$ is a \underline{metric} iff it satisfies
\begin{enumerate}
\item $d(x, y) = d(y, x)$
\item $d(x,y)\geq 0\,$, and $\,d(x,y) = 0 \Leftrightarrow x = y$
\item $d(x, z) \leq d(x, y) + d(y, z)$. 
\end{enumerate}
\end{definition}
Notion of the open ball: 
\[{B_r}\left( x \right) = \left\{ {x' \in X:d\left( {x,x'} \right) < r} \right\}.\]

Recall that every open set is the union of open sets. 

Notion of sequential properties, equivalence between a property and a sequential property in a metric space. 

\subsection*{Cauchy Sequences}
\begin{definition}
A sequence $n\mapsto u_n$ is a \underline{Cauchy sequence} iff for $\varepsilon >0$, there exists an $N>0$ such that for any $n, m>N$, $d(u_n, u_m) < \varepsilon$. 
\end{definition}
In other words, the terms in the sequence get progressively closer together. A space in which every Cauchy sequence converges is called a complete space. 
\begin{ex}
The set of rational numbers, $\Q$. We can generate a Cauchy sequence whose limit is $\sqrt{2}$, for instance. While the sequence itself may be Cauchy, it certainly does not converge in the space $\Q$ since $\sqrt{2} \notin \Q$. 
\end{ex}
Often, failure of a Cauchy sequence to converge is actually the fault of the space, not necessarily the sequence itself. 
\begin{note}
If a sequence $n\mapsto u_n$ converges to $u$, then the sequence $u_n$ is a Cauchy sequence. 
\end{note}
\begin{proof}
Let $X$ be a metric space, and suppose $u_n \to u$. Then, given $\varepsilon > 0$, there exists an $N$ such that for all $n,m>N$, $d(u_n, u) < \varepsilon/2$ and $ d(u_m, u) < \varepsilon/2$. But then 
\[d\left( {{u_n},{u_m}} \right) \leq d\left( {{u_n},u} \right) + d\left( {u,{u_m}} \right) \leq \varepsilon \]
which proves that the sequence is Cauchy. 
\end{proof}

\begin{thm}[Banach Contractive Map Theorem]
Let $X$ be a complete metric space, and let $F:X\to X$ be a contraction, that is 
\[d\left( {F\left( x \right),F\left( y \right)} \right) \leq Cd\left( {x,y} \right)\qquad C < 1.\]
Then there exists a unique fixed point $\tilde x\in X$ that satisfies $\tilde x = F\left(\tilde x\right)$. 
\end{thm}
\begin{proof}
Existence: use method of repeated iterations. 

Uniqueness: use contradiction. 
\end{proof}

\begin{definition}
A set is \underline{bounded} iff it is fully contained in some $B_r(x)$. 
\end{definition}

\begin{definition}
Let $X$ be any non-empty set and $Y$ a metric space. A function $f:X\to Y$ is bounded iff its range, $\mathcal R(f)$ is bounded in $Y$.  
\end{definition}

\begin{definition}
The space $\ell^\infty(X,Y)$ is the set of all bounded functions $f:X\to Y$, equipped with the metric $d(f,g) = \sup_{x\in X} d\left(f(x), g(x)\right)$. In particular, this generates the so-called topology of uniform convergence which we will study later. 
\end{definition}

\begin{ex}
  Consider 
  \[f_n(x) = \left( x + \frac{1}{n}\right)^2,\qquad f:\R\to\R.\]
  It is easy to see that the sequence converges pointwise, that is, $f_n(x) \to f(x) $ for all $x\in \R$, but does not converge uniformly since at very large $x$, the difference between $x^2$ and $(x-\frac{1}{n})^2$ may be very large. 
\end{ex}

\begin{prop}
  If $Y$ is complete, then $\ell^\infty (X, Y)$ is complete. 
\end{prop}
\begin{proof}
  Pick a Cauchy sequence $u_n\in \ell^\infty(X, Y)$. Since 
  \[d\left( {{u_n}\left( x \right),{u_m}\left( x \right)} \right) \leq \mathop {\sup }\limits_{x \in X} d\left( {{u_n}\left( x \right),{u_m}\left( x \right)} \right) = d\left( {{u_n},{u_m}} \right),\]
  then the sequence $n\mapsto u_n(x)$ is Cauchy in $Y$. Since $Y$ is complete, the limit $\lim u_n(x) = u(x)$ exists. This proves that $u_n\to u$ pointwise. It remains to be shown that $u_n$ converges to $u$ in the norm of the space, and that in fact $u\in \ell^\infty(X, Y)$. 
  Indeed, first pick $N$ such that $d(u_n, u_m)\leq \varepsilon/2$ for all $m,n>N$. Now since $u_n(x)\to u(x)$, pick $n>N$ that satisfies $d\left( {{u_n}\left( x \right),u\left( x \right)} \right) \leq \varepsilon /2$. Then 
  \begin{align*}
    d\left( {{u_n}\left( x \right),u\left( x \right)} \right) &\leq d\left( {{u_n}\left( x \right),{u_m}\left( x \right)} \right) + d\left( {{u_m}\left( x \right),u\left( x \right)} \right) \hfill \\
     &\leq \frac{\varepsilon }{2} + \frac{\varepsilon }{2} \hfill \\
     &\leq \varepsilon  
  \end{align*} 
  Taking the supremum on both sides tells us that $d(u_n u)\leq \varepsilon$ which proves that $u_n$ converges to $u$ in the $\ell^\infty(X,Y)$ metric. It is easy to see that $u$ is also bounded (why?). 
\end{proof}

On a similar note, assume that $X$ is now a metric space. Then, 
\begin{definition}
  $C(X,Y)$ denotes the set of all functions $f:X\to Y$ that are continuous, equipped with the metric 
  \[d(f,g) = \sup_{x\in X} d(f(x), g(x))\]
  and is itself a subspace of $\ell^\infty(X,Y)$. 
\end{definition}

\begin{prop}
  If $Y$ is complete, then $C(X,Y)$ is also complete. 
\end{prop}
\begin{proof}
  Take a Cauchy sequence $n\mapsto u_n$ in $C(X,Y)$. One needs to show that $u_n \to u$ in $C$. Now, since $C(X,Y)$ is a subspace of $\ell^\infty$ which we know is complete whenever $Y$ is complete, we know that the sequence $u_n$ converges to $u$ in $\ell^\infty$. All that remains to show then is that the limit $u$ lives in $C(X,Y)$. 
  That is, one needs to show that $d(u(x),u(y))<\varepsilon$ whenever $d(x,y)<\delta$. The general triangle path that we can down is the following: 
  \[d\left( {u\left( x \right),u\left( y \right)} \right) \leq d\left( {u\left( x \right),{u_n}\left( x \right)} \right) + d\left( {{u_n}\left( x \right),{u_n}\left( y \right)} \right) + d\left( {{u_n}\left( y \right),u\left( y \right)} \right).\]
  Now since $u_n\to u $ in the $\ell^\infty$ sense, given $\varepsilon>0$, there exists an $N$ such that $n\geq N\implies d(u_n, u)<\varepsilon /3$. 
  This implies that $d\left( {{u_n}\left( x \right),u\left( x \right)} \right) \leq \varepsilon /3.$ 

  Additionally, since $u_n\in C(X,Y)$, let us choose $\delta$ such that $d(u_n(x), u_n(y))<\varepsilon /3 $ whenever $d(x,y)<\delta$. 

  Putting everything together now using the original triangle path, we have that $d(u(x),u(y))<\varepsilon$ whenever $d(x,y)<\delta$ which proves that the limit $u$ is continuous. 
\end{proof}


\subsection{Integration}
Short section, see integration notes online for complete explanations. 

\begin{ex}
  $\Omega \subset \R^n$, $\mu$ is a Lebesgue measure on $\R^n$, $\mu(\Omega) > 0$. 
  The $L^p$ spaces, $1\leq p < \infty$ are spaces of functions satisfying 
  \[{L^p}\left( \Omega  \right) = \left\{ {f:\Omega  \to \mathbb{R}:\int\limits_\Omega  {{{\left| f \right|}^p}d\mu }  < \infty } \right\}.\]
  We also have a norm defined as 
  \[{\left\| f \right\|_p} = {\left\{ {\int\limits_\Omega  {{{\left| f \right|}^p}d\mu } } \right\}^{1/p}}.\]
  We comment that elements of $L^p$ are not really functions. They are equivalence classes of functions that differ on a set of measure zero. Recall that the Lebesgue integral does not see sets of measure zero, so two functions may differ at a countable number of points and will still have the same $p$-norm. 
\end{ex}



\subsection{Inequalities}
Also a short section. See notes online for complete explanations. 
Perhaps the two most important inequalities when dealing with $L^p$ spaces are the Minkowsi and H{\"o}lder's Inequalities. We have 
\[\begin{array}{*{20}{c}}
  {{{\left\| {f + g} \right\|}_p} \leq {{\left\| f \right\|}_p} + {{\left\| g \right\|}_p}}&{\text{Minkowski}} \\[.25cm] 
  {\displaystyle\int\limits_\Omega  {\left| {fg} \right|}  \leq {{\left\| f \right\|}_p}{{\left\| g \right\|}_q},\quad {p^{ - 1}} + {q^{ - 1}} = 1}&{\text{H\"older}} 
\end{array}\]

\section{Normed Linear Spaces}

\subsection{Definitions and Examples}
\begin{definition}
  $X$ is a vector space. A function $\left\|  \cdot  \right\|:X \to {\mathbb{R}^ + }$ is a \underline{norm} iff it satisfies 
\begin{enumerate}
  \item $\|u+v\|\leq \|u\| + \|v\|$
  \item $\|\lambda u\| = |\lambda |\|u\|$
  \item $\|u\| = 0 \Leftrightarrow u = 0$. 
\end{enumerate}
Functions satisfying only the first two properties are called \underline{seminorms}. 
\end{definition}
\begin{ex}
  $\R^n$, $x = (x_1, x_2, \dots, x_n)$. The functions $\rho_j(x) = |x_j|$ are seminorms. 
\end{ex}
\begin{ex}
  $\mathcal S(\R^n)$, the Schwartz space of test functions. 
\end{ex}
The norm on $X$ induces a metric on $X$, namely 
\[d(u,v)=\|u-v\|\]
so every normed space is a metric space. 

\begin{definition}
  A complete normed linear space is called a \underline{Banach space}. 
\end{definition}
\begin{ex}
  $\R^n$, $\|x\|_\infty = \sup_j |x_j|$, $\|x\|_p = \left\{\sum_j |x_j|^p\right\}^{1/p}$. 
\end{ex}
\begin{lemma}
  The two norms above are equivalent. 
\end{lemma}
\begin{definition}
  We say that two norms $\|\cdot\|_1$ and $\|\cdot\|_2$ are \underline{equivalent} iff there exists constants $c_1, c_2>0$ such that 
  \[{c_1}{\left\| x \right\|_1} \leq {\left\| x \right\|_2} \leq {c_2}{\left\| x \right\|_1}\] for all $x\in X$. 
  In essence, this definition means that both norms agree when elements in $X$ are small. Additionally, all norms are equivalent on finite-dimensional spaces. 
\end{definition}
\begin{ex}
  $\ell^\infty (X,\R)$, $C(X,\R)$. 
\end{ex}
\begin{definition}
  Let $X_1$, $X_2$ be normed linear spaces. Let $x = (x_1, x_2)$ where $x_i\in X_i$. Then $X_1 \times X_2$ is a normed linear space with $\|(x_1, x_2)\| = \max_j \|x_j\|$. 
\end{definition}
\begin{prop}
  If $X_1$ and $X_2$ are complete, then $X_1\times X_2$ is also complete. 
\end{prop}
\begin{definition}
  $f:X\to Y$ is \underline{uniformly continuous} iff for all $\varepsilon >0$ there exists a $\delta$, independent of $x$, such that $d(f(x),f(y)) < \varepsilon$ whenever $d(x,y)<\delta$. The key is that $\delta$ does not depend on $x$, only $\varepsilon$!
\end{definition}
\begin{prop} The following hold: 
  \begin{enumerate}
    \item $(x,y) \to x + y$ is uniformly continuous on $X\times X$. 
    \item $(\lambda, x)\to \lambda x$ is continuous on $\mathbb F \times X$. 
    \item $x \to \lambda x$ (fixed $\lambda$) is uniformly continuous on $X$. 
    \item $x\to \|x\|$ is uniformly continuous on $X$. 
  \end{enumerate}
\end{prop}
\begin{proof}
  Follows easily. Just compute and use (reverse) triangle inequality. 
\end{proof}

\begin{definition}
  A \underline{subspace} $X\subset Y$ of a normed linear space $Y$ is a vector space equipped with the same norm from $Y$. 
\end{definition}
\begin{ex}
  $\ell^\infty (S, Y)$ with $S$ a metric space. $C(S,Y)$ is a subspace of $\ell^\infty (S, Y)\subset Y^S$. 
\end{ex}
\begin{prop}
  If $X$ is a subspace of $Y$, then $\overline X $ is also a subspace of $Y$. 
\end{prop}
\begin{proof}
  Outline: Need to show that $\overline X $ is closed under addition and scalar multiplication. 
\end{proof}






\subsection{Other Loose Ends}
\subsection{Linear Maps}
\subsection{Duals, Hahn-Banach,...}

\begin{definition}
  The \underline{dual} of a normed linear space $X$ over $\mathbb F$ is $X'=L(X,\mathbb F)$. 
\end{definition}

\begin{note}
  All dual spaces are complete. If $\varphi \in X'$, then $x\mapsto |\varphi(x)|$ is a seminorm. 
\end{note}
\begin{lemma}
  Let $p,q\in [1,\infty]$ such that $1/p+1/q=1$. 
\end{lemma}


\subsection{Dual Operators}
\newpage \subsection{Baire and Consequences}

Let $X$ be a metric space. 
\begin{definition}
  $S\subset X$ is \underline{nowhere dense} iff $\overline S$ has no interior points. 
\end{definition}
\begin{definition}
  $M\subset X$ is said to be of \underline{first category} iff it can be written as a countable union of nowhere dense sets. Otherwise, it is said to be of \underline{second category. }
\end{definition}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\begin{thm}[Baire Category Theorem]
  Any complete metric space (i.e. a Banach Space) is of second category. That is, it can not be written as a countable union of nowhere dense sets. 
\end{thm}
\end{tcolorbox}
\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\begin{theorem}
  Let $X$ be complete, $S_1, S_2, \dots \subset X$ closed. If, $\bigcup {{S_n}}  = X$, then at least one $S_n$ contains an interior point. \color{gray}Equivalently, there is a point $x_0\in S_n$ and an $r>0$ such that I can fit a ball around $x_0$ which remains fully contained in $S_n$. \color{black}
\end{theorem}
\end{tcolorbox}
\begin{proof}
  Go by contradiction. Assume to the contrary that NONE of the $S_n$ possess an interior point. If that were the case, then you could take any $x\in X$ and draw a ball of any radius around it that does not fit in $S_n$. \color{gray} Equivalently, thinking about $x$ as NOT being an interior point of any $S_n$. \color{black} Equivalently, the intersection 
  \[B_\varepsilon(x) \bigcap S_n^c \neq \emptyset\]
  holds for all $x, \varepsilon, n$. 
  Now, we inductively construct $x_1, x_2,\dots$ and $\varepsilon_1, \varepsilon_2,\dots$ by first looking at $B_{\varepsilon_n}(x_n)\bigcap S_n^c$ and selecting $x_{n+1}$ and $\varepsilon_{n+1}$ so that \[B_{\varepsilon_{n+1}} (x_{n+1})\subset B_{\varepsilon_n}(x_n)\bigcap S_n^c,\qquad \varepsilon_{n+1}< \frac{1}{2}\varepsilon_n.\]
  If we continue, then we will generate a sequence $x_k$ and $\varepsilon_k \in (0, 2^{-k})$. We claim that the sequence $x_k$ is Cauchy. This follows since for any $m>n$, \[d(x_n, x_m)\leq \varepsilon_n = 2^{-n}\varepsilon_0.\] Therefore, $x = \lim x_k$ exists, moreover, $x\in B_{\varepsilon_{n+1}}(x_{n+1})\subset S^c_n$ for all $n$, so $x\notin \bigcup S_n = X$, a contradiction. 
\end{proof}
We remark that the consequences of Baire's Theorem are extremely useful and important. 
\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
  \begin{thm}[Uniform Boundedness Principle]
  Let $X$ be Banach, $Y$ a NLS. Let $\mathcal A $ be a family of linear operators $A\in L(X, Y)$. If \[\sup_{A\in \mathcal A} \|Ax\|< \infty \] for every $x\in X$, then 
  \[\sup_{A\in \mathcal A} \|A\| < \infty.\]
\end{thm}
\end{tcolorbox}
\cg Basically, UB connects \underline{pointwise boundedness} (i.e. for every $x\in X$) with boundedness in the \underline{operator norm.} \cbk
\begin{proof}
  Consider the sets 
  \[S_n = \left\{ x\in X : \sup_{A\in \mathcal A}\|Ax\|\leq n\right\} = \bigcap_{A\in \mathcal A} \left\{ x\in X : \|Ax\|\leq n\right\}. \]
  Notice firstly that the sets $S_n$ are closed since they are the (arbitrary) intersection of closed sets. Indeed, the sets on the right are closed as they are the inverse image of closed sets in $Y$. Furthermore, $X = \bigcup S_n$ and by Baire's Category Theorem, $X$ is of second category. 

  It follows also by prev. theorem that at least one $S_n$ contains an interior point. That is, there exists an $x_0, r, n$ such that $B_r(x_0) \subset S_n$. \cg $x_0$ is an interior point of some $S_n$. \cbk Now for $\|x\|=1$, we have that for every $A\in \mathcal A$, 

  \begin{align*}
    \|Ax\| &= \|\frac{1}{r}A(x_0 + rx - x_0)\| \\[.2cm]
    &\leq \frac{1}{r}\|A(x_0 + rx)\| + \frac{1}{r}\|Ax_0\| \\[.2cm]
    &\leq \frac{2n}{r}.
  \end{align*}
  Why is this the case? First of all notice that if $\|x\|=1$, then $x_0 + r x$ lies in the closed set $S_n$. By definition then, $\|A(x_0 + rx)\|\leq n$. Also, $x_0$ is clearly in $S_n$ and so $\|Ax_0\| \leq n$. 

  Notice the bound is independent of $A$ or $x$. Taking the supremum over $\mathcal A$, we see that 
  \[\sup_{A\in \mathcal A} \|A\| \leq \frac{2n}{r}. \]
\end{proof}
\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\begin{thm}[Banach-Steinhaus]
  In the Uniform Boundedness theorem, we may replace the condition that $\sup_{A\in \mathcal A} \|Ax\| < \infty \,\, \forall x\in X$ with the condition that the set  
  \[\left\{ {x \in X:\mathop {\sup }\limits_{A \in \mathcal{A}} \left\| {Ax} \right\| < \infty } \right\}\] is of second category to conclude that $\sup_{A\in \mathcal A} \|A\| < \infty.$
\end{thm}
\end{tcolorbox}
\cg Perhaps the condition by Banach-Steinhaus will be easier to show, or maybe it may be more natural depending on the problem? \cbk

\begin{ex}
  Let $X = C([0, 1])$ equipped with the sup-norm. Consider the family of functionals 
  \[\varphi_\varepsilon(x) = \frac{x(\varepsilon)-x(-\varepsilon)}{2\varepsilon}, \quad \varepsilon>0. \]
  Observe that 
  \[\left\| {{\varphi _\varepsilon }} \right\| = \mathop {\sup }\limits_{\left\| x \right\| = 1} \left| {\frac{{x\left( \varepsilon  \right) - x\left( { - \varepsilon } \right)}}{{2\varepsilon }}} \right| = \frac{1}{{2\varepsilon }}\mathop {\sup }\limits_{\left\| x \right\| = 1} \left| {x\left( \varepsilon  \right) - x\left( { - \varepsilon } \right)} \right| = \frac{1}{\varepsilon }\]
  \cg To get rid of the 2, notice that the way to maximize the argument of the supremum is if $x(\varepsilon) = 1$ and $x(-\varepsilon) = -1$. That way, $\|x\|$ is still equal to unity but the argument is now 2. \cbk
  We have shown that $\sup \|\varphi_\varepsilon\|$ is unbounded. Thus by the contrapositive of UB, the set of functions \[\left\{ {x \in X:\mathop {\sup }\limits_{{\varphi _\varepsilon }} \left\| {{\varphi _\varepsilon }\left( x \right)} \right\| < \infty } \right\}\] is of first category. \cg This set can also be interpreted as the set of functions $x$ that are differentiable at zero. $\varphi_\varepsilon(x)$ is simply the derivative at zero!\cbk
\end{ex}

\begin{ex}
  Suppose that $A_1, A_2, \dots \in L(X, Y)$ are such that $A_n(x) \to A(x)$ for every $x\in X$. By UB, $A = \lim_n A_n$ is linear and bounded \cg (why?) UB establishes the result for the whole family. \cbk, but this does NOT imply that $A_n \to A$ in the operator norm. 
\end{ex}

In preparation for the open mapping theorem, we begin a discussion on closed operators. Let us first recall some notions. 
\begin{itemize}
  \item A relation $f$ can be thought of a set a subset of $X\times Y$. Namely, those pairs $(x, y)$ such that $y = f(x)$. 
  \item The domain of $f$, denoted $\mathop{dom}f =\{x\in X: \exists y\in Y : y = f(x)\}$. 
  \item The range of $f$, denoted $\mathop{range}f = \{y\in Y: \exists x\in X: y = f(x)\}$. 
  \item We say that $f$ is a \textit{function} iff $(x, y_1) \in f$, $(x, y_2) \in f$, then $y_1 = y_2$. \cg(Notice this is not the same statement as $f$ is one-to-one since it does not exlude the possibility that $x_1$ and $x_2$ both map to the same $y$.)\cbk
  \item The inverse relation $f^{-1} = \{(y, x) : (x, y)\in f\}$. 
  \item If $f$ is a one-to-one function, then $f^{-1}$ is also a function. 
\end{itemize}

\begin{definition}
  $X,Y$ Banach, $X_0$ a dense subspace of $X$. We say that a linear operator $A:X_0 \to Y$ is \underline{closed} iff its graph is closed in $X\times Y$. \cg The graph of $A$, recall, is $\{(x, y) \in X\times Y : y = Ax\}$. \cbk 
\end{definition}
By closed in $X\times Y$, we mean that if $(x_n, y_n)\in G(A)$, $x_n \to x\in X$, $y_n \to y \in Y$, then $(x,y)\in G(A)$. \cg The usual notion of closed in terms of accumulation points of a set. Equivalently, if $x_n\in X_0$, $x_n \to X$, and $Ax_n\to y\in Y$, then $y = Ax$ and $x\in X_0$. \cbk
\begin{ex}
  The simplest example can be made when $X_0 = X$. Let $A \in L(X,Y)$. Then $A$ is closed. \cg Why? Both $X$ and $Y$ are Banach, so sequences converge and the limits are in the sets. Finally, $A$ is continuous. Apply the sequential definition of continuity. \cbk 
\end{ex}
\begin{ex}
  A more interesting example is the case of the derivative operator. Recall that derivative operators are not bounded! However, as we will show, they are closed. Let $X = C([0,1])$, $Y=\{y\in X : y(0) =0\}$. Consider the map $A: X \to Y$ given by 
  \[(Ax)(t) = \int \limits_0^t x(s) ds.\]
  The range of $A$ is given by all functions $\{y\in Y : y'\in X\}=:Y_0$. The map $A: X \to Y_0$ is onto and invertible. Its inverse $A^{-1}: Y_0 \to X$ is given by $A^{-1} y = y'$. Notice that $A$ is closed and $A^{-1}$ is closed \cred (why?) See \href{https://math.stackexchange.com/questions/214218/uniform-convergence-of-derivatives-tao-14-2-7}{Stackexchange post about convergence.} \cbk, but not bounded.  

\end{ex}
\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\begin{thm}
  Let $A,B$ be linear operators from a normed linear space $X$ to Banach $Y$. Assume that there exists constance $0\leq a < 1$ and $b>0$ such that 
  \[\left\| {Bx} \right\| \leq a\left\| {Ax} \right\| + b\left\| x \right\|.\]
  If $A$ is closed, then $A+B$ is closed also. 
\end{thm}
\end{tcolorbox}
\begin{proof}
  Homework problem...see solution.
\end{proof}
\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
  \begin{thm}
    $A:X_0 \to Y$ is closed iff $(X_0, \|\cdot\|_A)$ is complete, where $\|x\|_A = \|x\| + \|Ax\|$. 
  \end{thm}
\end{tcolorbox}
\begin{proof}
  Assume that $(X_0, \|\cdot\|_A)$ is complete. Let $X_0 \ni x_n \to x \in X$ and $Ax_n \to y \in Y$. Then $x_n$ is a Cauchy sequence with respect to $\|\cdot \|_A$. \cg why? Well, \[{\left\| {{x_n} - {x_m}} \right\|_A} = \left\| {{x_n} - {x_m}} \right\| + \left\| {A{x_n} - A{x_m}} \right\|\] and by assumption, $x_n$ is Cauchy in the original norm and $Ax_n$ is Cauchy in the original norm, so the RHS goes to zero. \cbk Since $X_0$ is complete, the limit $x\in X_0$ and $\|x-x_n\|_A \to 0$. But if that is the case, then \[\|Ax-Ax_n\|\leq \|A\|\|x-x_n\|\to 0\] and so $y=Ax$. 

  Assume now that $A$ is closed. Pick a Cauchy sequence $X_0 \ni x_n$ that is Cauchy with respect to $\|\cdot \|_A$. Then $x_n$ is Cauchy in $X$ and $Ax_n$ is Cauchy in $Y$. Since $X$ and $Y$ are complete, $x_n\to x \in X$, $Ax_n \to y \in Y$. Therefore, \[\|x-x_n\|_A = \|x-x_n\| + \|Ax-Ax_n\| \to 0\] and we conclude the result. 
\end{proof}

\begin{definition}
  We say that $f:X\to Y$ is an \underline{open mapping} iff if $U$ is open in $X$, then $f(U)$ is open in $Y$. \cg i.e. $f$ maps open sets to open sets. \cbk 
\end{definition}
Notice that if $f$ is an invertible open mapping, then $f^{-1}$ is continuous. Why? Well, $f$ is invertible by assumption, so $f^{-1}$ certainly exists. Then just apply definition of continuity. \cg i.e. $f^{-1}$ is continuous iff the inverse image of every open set is open (and closed set closed). In this case, the inverse image of $f^{-1}$ is $f$. \cbk
\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\begin{thm}[Open Mapping Theorem]
  Let $X,Y$ be Banach. Assume $A:X\to Y$ is linear and closed. \cg i.e. $A$ has closed graph \cbk Then if $A$ is onto, $A$ is an open map. 
\end{thm}
\end{tcolorbox}
The converse statement is obvious. \cg Why? Intuitively, if $A$ is open, it maps open sets to open sets, so you would map $B_1(0)_X$ to some ball in $Y$ that contains $0$. In particular, $A(B_1(0)_X)\supset B_\varepsilon(0)_Y$. Now use linearity to blow up the ball to cover the whole space. We have for every $y\in Y$ that $\frac{\varepsilon}{2}y/\|y\| \in B_\varepsilon(0)_Y$, so it must be the image of some $x \in B_1(0)$. Then 
\[A\left( {\frac{{2\left\| y \right\|}}{\varepsilon }x} \right) = \frac{{2\left\| y \right\|}}{\varepsilon }A\left( x \right) = \frac{{2\left\| y \right\|}}{\varepsilon }\frac{\varepsilon }{{2\left\| y \right\|}}y = y\]\cbk
\begin{proof}
  Assume that $A$ is onto. Define unit balls in each space 
  \[U = \left\{ {x \in X:\left\| x \right\| < 1} \right\},\qquad V = \left\{ {y \in Y:\left\| y \right\| < 1} \right\}\]
  \cred come back to this proof later...9/26 lecture\cbk 
\end{proof}
Again, we have several useful corollaries of the Open Mapping Theorem. 

\begin{tcolorbox}[colframe=red!75!black]
  \begin{corollary}
    Let $X$, $Y$ be Banach. If a linear operator $A: X\to Y$ is invertible and bounded, then so is $A^{-1}:Y\to X$. 
  \end{corollary}
\end{tcolorbox}

\begin{tcolorbox}[colframe=red!75!black]
  \begin{corollary}
    If $(X, \|\cdot\|_1)$ and $(X, \|\cdot\|_2)$ are both Banach spaces with $\|\cdot \|_2 \leq c \|\cdot\|_1$ for every $x\in X$, then the two norms are equivalent,\cg i.e. there exists a $c_2$ such that $\|\cdot \|_1 \leq c_2 \|\cdot \|_2$ for every $x\in X$. 
  \end{corollary}
\end{tcolorbox}
\begin{proof}
  Apply the previous corollary to the map 
  \[{I_d}:\left( {X,{{\left\|  \cdot  \right\|}_1}} \right) \to \left( {X,{{\left\|  \cdot  \right\|}_2}} \right).\]
  \cg Notice that we have two Banach spaces by assumption, and the identity map is invertible and bounded by assumption since $\|I_d x \|_2 = \|x\|_2 \leq c \|x\|_1.$ The corollary tells us that the map \[I_d^{ - 1}:\left( {X,{{\left\|  \cdot  \right\|}_2}} \right) \to \left( {X,{{\left\|  \cdot  \right\|}_1}} \right)\] is bounded, that is, there exists a $c_2$ such that $\|I_d^{-1}x\|_1 = \|x\|_1\leq c_2 \|x\|_2 $.\cbk
\end{proof}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
  \begin{thm}[Closed Graph Theorem]
    Let $X,Y$ Banach. If a linear map $A:X\to Y$ is closed, then $A$ is bounded. 
  \end{thm}
\end{tcolorbox}
\begin{proof}
  Similarly, apply the above corollary to the map \[{I_d}:\left( {X,\left\|  \cdot  \right\|_A} \right) \to \left( {X,{{\left\|  \cdot  \right\|}}} \right),\qquad \|x\|_A = \|x\| + \|Ax\|.\]
  \cg Similar process. Recall from a previous theorem that $A$ is closed iff the space $(X, \|\cdot\|_A)$ is complete, so our setup is good. For every $x\in X$, it holds by inspection that 
  \[\left\| x \right\| \leq {\left\| x \right\|_A} = \left\| x \right\| + \left\| {Ax} \right\|.\]
  The previous corollary tells us then that the two norms are equivalent, i.e. there exists a $C_2$ such that 
  \[\left\| x \right\| + \left\| {Ax} \right\| \leq {C_2}\left\| x \right\| \Rightarrow \left\| {Ax} \right\| \leq \left( {{C_2} - 1} \right)\left\| x \right\|\]
  which shows that $A$ is bounded. \cbk
\end{proof}
We have a corollary:
\begin{tcolorbox}[colframe=red!75!black]
  \begin{corollary}
    Let $X$ be Banach. Assume that $U,V$ are closed subspaces where $U$ and $V$ are algebraically complemented. If $X = U+V$ and $U\cap V = \{0\}$, then the following hold: 
    
    \begin{enumerate}[label=(\alph*)]
      \item Every $x\in X$ admits a unique decomposition $x = u + v$, where $u\in U$ and $v\in V$, and the projections $P_U: X \to U$ and $P_V : X\to V$ are linear. 
      \item $U$ and $V$ are topologically complemented, that is, the projections are continuous. 
    \end{enumerate}

  \end{corollary}
\end{tcolorbox}
\begin{proof}
  See homework 6. 
\end{proof}
We complete now the proof of previous theorem: 
\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
  \begin{thm}
    Let $X,Y$ Banach and $A:X\to Y$ bounded. If $A' : Y' \to X'$ has bounded inverse, so does $A$. 
  \end{thm}
\end{tcolorbox}
\begin{proof}
  By the forward result, we see that $A''$ has a bounded inverse. In particular, since it is invertible, it is one-to-one. So, $A = {\left. {A''} \right|_{J\left( X \right)}}$ is one-to-one. \cg Here, $J$ is the canonical map from $X$ into $X''$ given by $J(x) = E_x\in X''$. \cbk To complete the proof, one needs to show that $A$ is onto. To do so, we establish two results: 
  \begin{enumerate}
    \item $AX$ is dense in $Y$. 
    
      Let $\psi \in (AX)^\circ$. Then $\psi(AX) = A'\psi X = 0$. So $A'\psi = 0 $ so $\psi = 0$ since $A' $ is one-to-one. Therefore, $(AX)^\circ = 0$ which implies that $AX$ is dense in $Y$. (Recall proposition from HB)

    \item $AX$ is closed, so $AX = Y$. 
    
      We know that $A''$ has a bounded inverse, so $A''$ maps open sets to open sets. \cg why? If $A$ is bounded, then $A''$ is also bounded by definition. By definition, any bounded operator is closed. Therefore, $A''$ is an open map by a previous corollary. \cbk We conclude then that $AX = A''X$ is closed. 
  \end{enumerate}
\end{proof}













\newpage \subsection{Weak Topologies} We have so far had only the notion of strong topologies. In fact, we can loosen our definitions slightly and introduce so called \textit{weak} topologies. 

\begin{ex}
  The topology of pointwise convergence on $Y = \mathbb F ^*  = \{f: X \to \mathbb F\}$ which is given by the seminorms $\rho_x(f) = |f(x)|$. Notice that this is the smallest topology that makes evaluations (i.e. $f\mapsto f(x)$) continuous operations. \cred what the hell does that mean?? \cbk
\end{ex}
\begin{note}
  If we have a \underline{countable} family $\mathcal R = \{\rho_1, \rho_2,\dots\}$ of seminorms, then the space $(Y, \mathcal R)$ is metrizable with metric 
  \[d(f, g) = \sum\limits_{n=1}^\infty\frac{\rho_n(f-g)}{1+\rho_n(f-g)}.\]
\end{note}
\begin{ex}
  Schwartz space of test functions $\mathcal S(\R^n)=f:\R^n \to \C$ that are smooth and decay rapidly. A possible semi-norm is 
  \[\|f\|_k = \max_{|\alpha|\leq k}\max_{|\beta|\leq k}\max_{x\in \R^n}|x^\beta D^\alpha f(x)|.\]
\end{ex}
\begin{definition}[Weak* Topology on $X'$]
  Let $X$ be a normed linear space and $X'$ its dual. The \underline{weak* topology on $X'$} is the topology of pointwise convergence.
  \begin{enumerate}
    \item It is equipped with the seminorms $\rho_x(\varphi) = |\varphi(x)|$ for every $x\in X$. 
    \item It is the smallest topology that makes evaluation (i.e. the map $X'\ni \varphi \mapsto \varphi(x)\in \mathbb F$) a continuous operation. \cg Note that the evaluation mapping is actually a member of the double dual, since it takes in $\varphi \in X'$ and spits out a number. \cbk
  \end{enumerate}
  We say that $\varphi_n\in X' $ converges weak* to $\varphi \in X'$, denoted $\varphi_n \stackrel{\ast}{\rightharpoonup} \varphi$ or $\varphi_n \xrightarrow{w*} \varphi$, if and only if $\varphi_n(x)\to \varphi(x)$ for every $x\in X$. \cg The space $X'$ is equipped with the weak* topology, so what we are really just restating is what it means for $\varphi_n$ to converge to $\varphi$ with respect to the seminorms $\rho_x$. Indeed, $\varphi_n$ converges to $\varphi$ with respect to $\rho_x$ iff by definition $\rho_x(\varphi_n - \varphi ) = |\varphi_n(x) - \varphi(x)| \to 0$ for every $x\in X$.  \cbk
\end{definition}
In fact, we have the following proposition. 
\begin{prop}
  \underline{Every} weak* continuous linear functional on $X'$ is of the form $\varphi \mapsto \varphi(x)$ for some $x\in X$. 
\end{prop}

\begin{note}
  The weak* topology on $X'$ is a subset of the strong (ordinary) topology on $X'$ \cred in what sense? why?\cbk, so we have the following implications: 

  \begin{enumerate}
    \item Weak* open/closed implies strongly open/closed. \cg why? \cbk
    \item Strong convergence implies weak* convergence. \cg why? if $\varphi_n$ converges strongly to $\varphi$, then for any continuous functional $E$ on $X'$, $E(\varphi_n) \to E(\varphi)$ by the definition of continuty. But, that is exactly the statement that $\varphi_n\rightharpoonup\varphi$.   \cbk
    \item Weak* continuity implies strong continuity. \cg why? \cbk
  \end{enumerate}

\end{note}

\begin{note}
  Let $X$ be Banach. By UB, a weak* convergent sequence is bounded \cred why? \cbk, in fact, if $\varphi_n \stackrel{\ast}{\rightharpoonup} \varphi$, then 
  \[\|\varphi\|\leq \liminf_n \|\varphi_n\|\]
  since
  \[\left\| \varphi  \right\| = \mathop {\sup }\limits_{\left\| x \right\| = 1} \left| {\varphi \left( x \right)} \right| = \mathop {\sup }\limits_{\left\| x \right\| = 1} \mathop {\lim }\limits_{n \to \infty } \left| {{\varphi _n}\left( x \right)} \right| = \mathop {\sup }\limits_{\left\| x \right\| = 1} \mathop {\lim \inf }\limits_{n \to \infty } \left| {{\varphi _n}\left( x \right)} \right| \leqslant \mathop {\lim \inf }\limits_{n \to \infty } \left\| {{\varphi _n}} \right\|\]
\end{note}
\begin{lemma}
  Let $X$ be Banach. Let $\varphi_n \stackrel{\ast}{\rightharpoonup} \varphi \in X'$. Then the sequence $\varphi_n$ is bounded. 
\end{lemma}
\begin{proof}
  Let $\mathcal A$ be the family of functionals $\varphi_n \in X'$ that converge weak* to $\varphi \in X'$. We claim that \[\mathop {\sup }\limits_{{\varphi _n} \in \mathcal{A}} \left\| {{\varphi _n}\left( x \right)} \right\| < \infty \] which follows since we know that for every $x\in X$, $\varphi_n(x) \to \varphi(x)$.\cg elaborate more? is there anything else to say here?\cbk By UB, we conclude that \[\mathop {\sup }\limits_{{\varphi _n} \in \mathcal{A}} \left\| {{\varphi _n}} \right\| < \infty \]
  that is, the sequence $\varphi_n$ is bounded. \cred 
  \href{https://math.berkeley.edu/~sarason/Class_Webpages/solutions_202B_assign13.pdf}{Ref1}, \href{https://heil.math.gatech.edu/handouts/weak.pdf}{Ref2}, \href{https://math.stackexchange.com/questions/1368767/do-we-need-completeness-for-a-weak-convergent-sequence-to-be-bounded}{Ref3}. \cbk 
\end{proof}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
  \begin{thm}[Banach-Alaoglu]
    The ball \[B_r = \{\varphi \in X' : \|\varphi\|\leq r\}\] is weak* compact. 
  \end{thm}
\end{tcolorbox}
\begin{proof}
  \cred See later...\cbk
\end{proof}
\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\begin{theorem}
  Assume $X$ is a separable NLS. \cg i.e. it contains a countable, dense subset. \cbk If $K\subset X'$ is a weak* compact set, then $K$ is metrizable in the weak* topology. That is, sequential compactness is equivalent to compactness. \cg What does it mean for a set to be weak* compact? The key significance is that we are now in a metric space. In a metric space, sequential notions are equivalent to their regular notions. \cbk
\end{theorem}
\end{tcolorbox}
\begin{tcolorbox}[colback=gray!5!white,colframe=red!75!black]
\begin{corollary}
  If $X$ is separable, then any bounded sequence in $X'$ has a weak* convergent subsequence. 
\end{corollary}
\end{tcolorbox}

We now switch to discussing the weak topology on $X$. 

\begin{definition}[Weak Topology on $X$]
  The \underline{weak topology on an NLS $X$} is given by seminorms $\rho_\varphi$, where 
  \[x\longmapsto \rho_\varphi(x)=|\varphi(x)|,\quad \varphi \in X'.\] 
  \begin{enumerate}
    \item We say that $x_n$ converges weakly to $x\in X$, denoted $x_n \rightharpoonup x$ if and only if $\varphi(x_n)\to \varphi(x)$ for every $\varphi \in X'$. \cg Again, this aligns with our established notion of weak topology on $X'$. In general, we say that a sequence $x_n$ converges weakly iff we act on it with any functional in the dual, that sequence converges (strongly). Contrast this with our definition of weak* convergence laid out earlier. Note that weak* convergence does not even make sense for guys in $X$! The whole point of weak* is that we are evaluating the functional. Such an analoge does not exist here. \cbk
    \item It is the weakest topology that makes all $\varphi \in X'$ continuous. \cred again, some clarification is needed here...\cg one way to think about it is the map $x\mapsto |\varphi(x)|$ is a member of the dual space, so you are just arguing here that the map is continuous, which is what the first point essentially says. \cbk 
  \end{enumerate}
\end{definition}

\begin{lemma}
  A weakly convergent sequence $x_n$ is bounded. 
\end{lemma}
\begin{proof}
  Similar reasoning with UB. Suppose that $x_n\rightharpoonup x$. For each $x_n$, identify the corresponding evaluation map $E_{x_n}\in X''$, where $E_{x_n}\varphi = \varphi (x_n)$. Let $\mathcal A$ be the family of such $E_{x_n}$. Notice that for each $\varphi \in X'$, the sequence $\varphi(x_n)$ converges \cg simply by the definition of the weak convergence of $x$! \cbk but the sequence $\varphi(x_n) = E_{x_n}\varphi$, so we see that pointwise \cg where here the points are regarded as functionals in the dual space\cbk, 
  \[\mathop {\sup }\limits_{{E_{{x_n}}} \in \mathcal{A}} \left\| {{E_{{x_n}}}\varphi } \right\| < \infty \]
  which, by UB, implies that \[\mathop {\sup }\limits_{{E_{{x_n}}} \in \mathcal{A}} \left\| {{E_{{x_n}}}} \right\| < \infty .\] The final step is to argue that $\|x\| = \|E_{x_n}\|$. This can be done by recognizing that the canonical map $X \ni x \to E_x \in X''$ is an isometry, which we prove as follows. 
  \[\left\| {{E_{{x_n}}}} \right\| = \mathop {\sup }\limits_{\left\| \varphi  \right\| = 1} \left\| {{E_{{x_n}}}\varphi } \right\| = \mathop {\sup }\limits_{\left\| \varphi  \right\| = 1} \left\| {\varphi \left( {{x_n}} \right)} \right\| \leqslant \mathop {\sup }\limits_{\left\| \varphi  \right\| = 1} \left\| \varphi  \right\|\left\| {{x_n}} \right\| = \left\| {{x_n}} \right\|.\]
  On the other hand, by HB, we know that for every $x\in X$, there exists a functional $\phi \in X'$ such that $\phi (x) = \|x\|$ and $\|\phi\|=1$. Then, 
  \[\left\| {{E_{{x_n}}}} \right\| = \mathop {\sup }\limits_{\left\| \varphi  \right\| = 1} \left\| {{E_{{x_n}}}\varphi } \right\| = \mathop {\sup }\limits_{\left\| \varphi  \right\| = 1} \left\| {\varphi \left( {{x_n}} \right)} \right\| \geqslant \left\| {\phi \left( {{x_n}} \right)} \right\| = \left\| {{x_n}} \right\|\]
  which proves that $\|x\|=\|E_x\|$. 
\end{proof}

\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black]
  Quick summary of topologies on $X'$: we say that a sequence $\varphi_n \in X'$ converges 

  \[\begin{array}{*{20}{c}}
    {{\text{strongly to }}\varphi :}&{{\varphi _n} \to \varphi }& \Leftrightarrow &{\mathop {\lim }\limits_{n \to \infty } \left\| {{\varphi _n} - \varphi } \right\| = 0} \\ 
    {{\text{weakly to }}\varphi :}&{{\varphi _n} \rightharpoonup \varphi }& \Leftrightarrow &{\mathop {\lim }\limits_{n \to \infty } E\left( {{\varphi _n}} \right) = E\left( \varphi  \right){\text{ for every }}E \in X''} \\ 
    {{\text{weak* to }}\varphi :}&{{\varphi _n}\mathop  \rightharpoonup \limits^ *  \varphi}& \Leftrightarrow &{\mathop {\lim }\limits_{n \to \infty } {\varphi _n}\left( x \right) = \varphi \left( x \right){\text{ for every }}x \in X} 
  \end{array}\]

\end{tcolorbox}
We also see that $\textit{weak}*\subset \textit{weak} \subset \textit{strong}$. \cg Again, how to interpret this? \cbk

\cg One may wonder what the differences are between the weak topologies and the weak* topologies. For instance, is $E\in X''$ not simply an evaluation? Well, in the general case, no. We may in general have more in the bidual than the original space. If, however, the space is reflexive, then the notions of weak and weak* convergence coincide. \cbk

\begin{tcolorbox}[colback=gray!5!white,colframe=red!75!black]
\begin{corollary}
  If $X$ is reflexive, then any ball $B_r(0) = \{x\in X : \|x\|\leq r\}$ is compact in the weak topology. In addition, if $X$ is separable, then any bounded sequence in $X$ has a weakly convergent subsequence. \href{http://mathonline.wikidot.com/every-bounded-sequence-in-a-reflexive-space-x-has-a-weakly-c}{See.}
\end{corollary}
\end{tcolorbox}













\newpage \subsection{Spectral Theory I}

Let $X$ be a Banach space over $\C$ and $X_0$ be a dense subspace of $X$. 

\begin{definition}
Assume that $A: X_0 \to X$ is linear. The \underline{resolvent set}, $\varrho(A)$, consists of all $z\in \C$ satisfying the following two conditions: 
\begin{enumerate}
\item $Y = \mathcal R(A-zI)$ is dense in $X$. 
\item $A-zI:X_0 \to Y$ has a bounded inverse. 
\end{enumerate}

The \underline{resolvent} of $A$ is the function 
\[z\mapsto (A-zI)^{-1}, \qquad z\in \varrho(A).\]

The \underline{spectrum} of $A$ is the complement of the resolvent set. 
\[\sigma(A) = \C\setminus \varrho(A).\]
Fact: the spectrum is closed. 

\end{definition}

\begin{ex}
Eigenvalues of a matrix. Indeed, the eigenvalues satisfy 
\[(A-\lambda I)x = 0, \qquad x \neq 0\]
and the belong in the spectrum. In some sense, the spectrum is a generalization of the traditional notion of eigenvalues. 
\end{ex}
















\newpage \subsection{Compact Operators}

\cg \textit{Some quotes from 10/12: ``not much to it, just some math'', ``it is what it is'', ``ban complex numbers'', ``can't dial an imaginary number'', ``it's not rocket science''.} \cbk 

\begin{definition}
  Let $X,Y$ be normed linear spaces. A linear operator $A: X\to Y$ is \underline{compact} if and only if it maps bounded sests to sets whose closure is compact. \cg also called \textit{precompact} sets. \cbk 
\end{definition}

\begin{note}
  Equivalently, compact operators map bounded sequences to sequences that have convergent subsequences. \cg notice that a bounded sequence comes from a bounded set, so you're just selecting a specific sequence within that bounded set. Apply the definition of compact operator and the result is a precompact set, which by definition, when closed, is compact. In the language of sequences, it means your image is going to have at least a convergent subsequence.\cbk 
\end{note}

\begin{note}
  Any linear operator $A:X\to \mathbb F^n$ that is bounded is compact. \cg why? well, the key is that the range is finite dimensional, so we can apply the Bolzano-Weierstrass Theorem. For any bounded sequence $x_n \in X$, its image $Ax_n$ is a bounded sequence in $\mathbb F^n$, which, by BW, contains a convergent subsequence. \cbk More generally, we claim that any bounded, \underline{finite-rank} \cg i.e. the range of $A$ is of finite dimension, \cbk operator is compact. 
\end{note}


\begin{prop}
  Let $X,Y,Z$ be Banach. Denote by $\mathcal K(X,Y)$ the set of all compact operators from $X$ to $Y$. 
  \begin{enumerate}
    \item $\mathcal K(X,Y)$ is a closed subspace of $L(X,Y)$.
    \item Let $A\in L(X,Y)$ and $B\in L(Y,Z)$. If either $A$ or $B$ is compact, then so is $BA$. \cg Composition of a compact operator with a bounded operator remains compact. \cbk
  \end{enumerate}
\end{prop}
\begin{proof}
  asdf
\end{proof}

\begin{note}
  If $A$ can be written as the limit of finite rank operators, then $A$ is compact. \cg Side note: the approximation property (AP) for Banach spaces provides the converse which is not always true. We say that a Banach space has the approximation property if every compact operator is a limit of finite-rank operators. 
\end{note}
\cred is there a proof for this? \cbk 
\cblu couple of examples omitted here...\cbk 





\cg (10/14 Lecture:) \cbk

Let $X$ be a compact topological space, $Y$ a metric space. 
\begin{definition}
  A family of functions $\mathcal F \subset C(X,Y)$ is \underline{equicontinuous} if for every $x_0 \in X$ and for every $\varepsilon > 0$ there exists an open set $\mathcal U$ containing $x_0$ such that $d\left( {f\left( x \right),f\left( {{x_0}} \right)} \right) < \varepsilon $ whenever $x\in \mathcal U$ and $f\in \mathcal F$. 
\end{definition}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\begin{thm}[Arzel\`a-Ascoli]
  Such a family $\mathcal F$ has compact closure if 
  \begin{enumerate}
    \item $\mathcal F$ is equicontinuous
    \item For each $x\in X$, the set 
    \[\mathcal{F}\left( x \right) = \left\{ {f\left( x \right):f \in \mathcal{F}} \right\}\] 
    has compact closure. 
  \end{enumerate}
\end{thm}
\end{tcolorbox}
\begin{proof}
  See notes...one can do with Tychonoff which is much shorter than without. 
\end{proof}
\begin{ex}
  Let $\Omega \subset \R^d$ be compact and nonempty, let $K\in C(\Omega \times \Omega)$. For any $f\in L^1 (\Omega)$, set 
  \[\left( {Af} \right)\left( y \right) = \int\limits_\Omega  {K\left( {y,x} \right)f\left( x \right)dx} .\]
Then we have 
\begin{align*}
  \left| {Af\left( y \right) - Af\left( {{y_0}} \right)} \right| &= \left| {\int\limits_\Omega  {K\left( {y,x} \right)f\left( x \right)dx}  - \int\limits_\Omega  {K\left( {{y_0},x} \right)f\left( x \right)dx} } \right| \hfill \\
   &= \left| {\int\limits_\Omega  {\left( {K\left( {y,x} \right) - K\left( {{y_0},x} \right)} \right)f\left( x \right)dx} } \right| \hfill \\
   &\leq {\left\| f \right\|_1}\mathop {\sup }\limits_{x \in \Omega } \left| {K\left( {y,x} \right) - K\left( {{y_0},x} \right)} \right|. 
\end{align*} 
But since $\Omega$ is compact and $K$ is continuous, $K$ is uniformly continuous, so \[\left| {K\left( {y,x} \right) - K\left( {{y_0},x} \right)} \right| \to 0 \] as $y\to y_0$. So, the family $\left\{ {Af:{{\left\| f \right\|}_1} \leqslant 1} \right\}$ is equicontinuous. (Choice of 1 as the radius is not particularly important, just a demonstration.)

Consequently, $A: L^1(\Omega) \to C(\Omega)$ is compact. Since the inclusion $C(\Omega) \to L^1(\Omega)$ is bounded, that is, ${\left\| f \right\|_1} \leqslant \left| \Omega  \right|{\left\| f \right\|_\infty }$, the maps $A: C(\Omega) \to C(\Omega)$ and $A: L^1(\Omega) \to L^1(\Omega)$ are also compact since the composition of a compact map with a bounded one is still compact. 
\end{ex}
\begin{definition}
  For $\Omega \subset \R^d$, equip the space of $n$ times continuously differentiable functions $C^n(\Omega)$ with the norm 
  \[{\left\| f \right\|^{\left( n \right)}}: = \mathop {\max }\limits_{\left| \alpha  \right| \leq n} {\left\| {{D^\alpha }f} \right\|_\infty }.\]
\end{definition}
\begin{propp}
  The inclusion map 
  \[A_n : C^{n+1} (\Omega) \to C^n(\Omega) \] is compact. Here, $\Omega = [0,1]$ or $[a,b]$. 
\end{propp}
\begin{proof}
  First consider $n=0$: we have that 
  \[\left| {f\left( x \right) - f\left( {{x_0}} \right)} \right| \leqslant {\left\| f \right\|^{\left( 1 \right)}}\left| {x - {x_0}} \right|\]
  so the set $\left\{ {f \in {C^1}:{{\left\| f \right\|}^{\left( 1 \right)}} \leqslant 1} \right\}$ is equicontinuous in $C^0$. So, the unit ball in $C^1$ has compact closure. 

  For $n>1$, we go by induction and apply the $n=0$ result. We consider the maps 
  \[{C^{n + 1}} \ni f \longmapsto \mathop {\left[ {\begin{array}{*{20}{c}}
    f \\ 
    {f'} \\ 
     \vdots  \\ 
    {{f^{\left( n \right)}}} 
  \end{array}} \right]}\limits_{ \in {C^1} \times  \ldots  \times {C^1}}  \longmapsto \mathop {\left[ {\begin{array}{*{20}{c}}
    {A_nf} \\ 
    {A_nf'} \\ 
     \vdots  \\ 
    {A_n{f^{\left( n \right)}}} 
  \end{array}} \right]}\limits_{ \in {C^0} \times  \ldots  \times {C^0}}  \longmapsto f \in {C^n}.\]
  The first and last maps are bounded and $A_n$ is compact (as shown for $n=0$), so the result is also compact.We remark the same holds for compact $\Omega \in \R^d$.  
\end{proof}


\subsection*{Spectral Stuff...}
\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\begin{theorem}
  Let $X$ be Banach, $A\in \mathcal K(X,X)$, $\lambda\neq 0$. Then $\mathcal N(A-I)$ is finite dimensional and $\mathcal R(A-I)$ is closed. 
\end{theorem}
\end{tcolorbox}
\begin{proof}
  Homework...
\end{proof}
\begin{tcolorbox}[colback=gray!5!white,colframe=red!75!black]
\begin{corollary}
  Under the same conditions, $\mathcal N\left( (\lambda I -A)^n\right)$ is finite dimensional and $\mathcal R\left( (\lambda I -A)^n\right)$ is closed.  
\end{corollary}
\end{tcolorbox}
\begin{proof}
  \[{\left( {\lambda I - A} \right)^n} = {\lambda ^n}{\left( {I - {\lambda ^{ - 1}}A} \right)^n} = {\lambda ^n}\left( {I + \underbrace {\sum\limits_{k = 1}^n {{{\left( { - \lambda } \right)}^{ - k}}{A^k}} }_{ = :B}} \right)\] follows from the binomial theorem and then apply the previous result with $B$. 
\end{proof}
\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\begin{theorem}
  The eigenvalues of $A\in \mathcal K (X)$ are countable in number and accumulate at most at zero. That is, if they accumulate, then they must accumulate at zero. 
\end{theorem}
\end{tcolorbox}
\begin{proof}
  Let $r>0$, let $E_r=\{u_1, u_2, u_3, \dots\}$ be a linearly independent collection of eigenvectors with eigenvalues $|\lambda_n|\geq r$. Consider the nested subspaces $U_n = \text{span} \{u_1,\dots, u_n\}$. Notice that $(A-\lambda_n I) U_n \subset U_{n-1}$ \color{red}(why?)\color{black} Now pick $x_n \in X$ such that $x_n \in U_n$, $\|x_n\|=1$, and $d(x_n, U_{n-1}) \geq 1/2$, say. Now we have for $n>m$ that 
  \[Ax_n - Ax_m = \lambda_n x_n + (A-\lambda_n) x_n - Ax_m.\] Now call $(A-\lambda_n) x_n - Ax_m =: -\lambda_n x\in U_{n-1}$ and then 
  \[\left\| {A{x_n} - A{x_m}} \right\| = \left| {{\lambda _n}} \right|\left\| {{x_n} - x} \right\| \geq \frac{r}{2}.\]
  Look at the sequence $n\mapsto Ax_n$ and conclude that it has to stop since $n\mapsto Ax_n$ has a convergent subsequence, so $E_r$ must be a finite set. 
\end{proof}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\begin{theorem}[Fredholm Alternative]
  Let $A \in \mathcal K (X)$ with $X$ Banach. Let $\lambda \neq 0$. Then either $\lambda$ is an eigenvalue or operator $(A-\lambda I)$ has bounded inverse. 
\end{theorem}
\end{tcolorbox}
\begin{proof}
  Consider the nested subspaces $X_0 = X$, $X_1 = (A-\lambda I )X_0$, $X_{n+1} = (A-\lambda I)X_n$. \cg note that $X_n = (A-\lambda I )^nX$. \cbk As long as $X_{n+1} \neq X_n$, pick a point $x_n \in X_n$ with $\|x_n\|=1$, and $d(x_n, X_{n+1})\geq 1/2$, say. Now if $m>n$, we have 
  \[Ax_n - Ax_m = \lambda x_n - \lambda x_m + (A-\lambda) x_n - (A-\lambda) x_m =: \lambda x_n - \lambda x\] \cg notice that the terms $(- \lambda x_m + (A-\lambda) x_n - (A-\lambda) x_m)\in X_{n+1}$.\cbk So, 
  \[\|Ax_n - Ax_m\| = \|\lambda (x_n - x)\| = |\lambda| \|x_n - x\|\geq \frac{|\lambda|}{2}.\]\cg notice that $\|x_n -x\|\geq 1/2$ since by assumption, the distance from $x_n$ to (any) a point in $X_{n+1}$ is no less than $1/2$. \cbk This cannot go on forever, since $Ax_n$ has a convergent subsequence. \cg notice that the sequence $x_n$ is bounded. Indeed, we chose $x_n$ to be of norm one, so the image of $x_n$ through $A$ must have a convergent subsequence since $A$ is compact. \cbk So, there must eventually be a point where $X_{n+1} = X_n$ for some $n$. That is, \[(A-\lambda I )^{n+1} X =(A-\lambda I )(A-\lambda I )^{n} X=(A-\lambda I )^{n} X.  \]
  Therefore, given any $y\in X$, there exists an $x\in X$ such that 
  \[(A-\lambda I )^{n+1} x = (A-\lambda I )^{n}y\]\cg why? well, beyond this critical number $n$, we have equality of the subspaces.\cbk 

  The theorem gives us two options, either $\lambda$ is an eigenvalue or it is not. If $\lambda$ is an eigenvalue, then $A-\lambda$ is not one-to-one and \cg what can we say...?\cbk. On the other hand, if $\lambda$ is not an eigenvalue, then $A-\lambda$ is one-to-one and thus so is $(A-\lambda)^n$. Rewrite this as 
  \[(A-\lambda)^n((A-\lambda)x-y)=0,\] from which you can conclude that $(A-\lambda)x = y$ and henceforth that $A-\lambda$ is onto. \cg remember, we took an arbitrary $y\in X$ to begin with.\cbk We now have an invertible map that is bounded \cg all compact operators are bounded. \cbk and by corollary of open mapping, then we have a bounded inverse. 
\end{proof}
\begin{note}
  With the same threshold $n$, 
  \[\mathcal R((A-\lambda)^{n+1}) = \mathcal R ((A-\lambda)^n) =: U = \bar U\]
  Let $V = \mathcal N ((A-\lambda)^n)$. We claim that $U,V$ are invariant under $A$. \cg not totally sure what the point is here...\cbk  
\end{note}




















\newpage \section{Hilbert Spaces}
\subsection*{Basics}
\begin{definition}
  An inner product space (IPS) is a vector space $X$ over $\mathbb F \in \{\R,\C\}$ with an inner product $\left( \cdot , \cdot \right) : X\times X \to \C$ satisfying 
  \begin{enumerate}
    \item $\left\langle z, x + cy\right\rangle = \langle z, x\rangle + c \langle z, y\rangle$
    \item $\langle x, z\rangle = \overline{\langle z, x\rangle}$
    \item $\langle x, x \rangle \geq 0$, and $\langle x, x \rangle = 0$ iff $x=0$. 
  \end{enumerate}
\end{definition}

\begin{note}
  $\langle x+cy, z\rangle = \langle x, z\rangle + \overline c \langle y, z\rangle$. 
\end{note}
\begin{note}
  $\|x\| = \sqrt{\langle x , x\rangle }$ defines a norm on $X$, so the space is a normed linear space with a norm inherited from the inner product. Proof of this statement to follow later...
\end{note}
\begin{definition}
  A Hilbert Space is a complete inner product space. 
\end{definition}
\begin{ex}
  $L^2(\Omega, d\mu)$ with the inner product $\langle x, y\rangle = \displaystyle \int_\Omega \overline x y d\mu$. One special case is $\ell^2(S)$ with the counting measure and $\langle x, y \rangle = \sum_s \overline{x(s)}y(s)$. Another special case is when $S=\{1, 2, \dots, n\}$ and then we recover the canonical inner product $\langle x, y \rangle = \sum\limits_{i=1}^n \overline{x_i}y_i$. 
\end{ex}

\begin{ex}
  Space $\mathbb M^{m\times n}$ of $m\times n$ matrices, with the inner product $\langle A,B\rangle = \text{tr}(A^*B)$. Here, $A^* = \overline{A'}$. 
\end{ex}
\begin{ex}
  Sobolev space $H^s(\mathbb T^n)$. See homework. 
\end{ex}
\begin{propp}[Cauchy-Schwarz]
  \[|\langle x, y \rangle| \leq \|x\|\|y\|.\]
\end{propp}
\begin{proof}\color{red}
  The statement is trivial if $x=0$ or $y=0$. Otherwise, assume that $\|x\|=\|y\|=1$. Then ...\color{black}
\end{proof}
We can also define the angle between two vectors as $\cos(\theta) = \langle x, y\rangle / \|x\|\|y\|$. 
We say that two vectors are orthogonal, denoted $x\perp y$ iff $\langle x, y\rangle = 0$. Notice that $x\perp x \leftrightarrow x = 0$. 

\begin{propp}  
  \begin{enumerate}[label=(\alph*)]
    \item $\|x+y\|^2 = \|x\|^2 + \|y\|^2 + 2Re\langle x, y \rangle$ (Remark: leads to Pythagoras when $x$ and $y$ are orthogonal.)
    \item $\|x+y\|\leq \|x\| + \|y\|$ (Remark: The triangle inequality shows that $\|\cdot \|:= \sqrt{\langle \cdot, \cdot \rangle}$ is indeed a norm.)
  \end{enumerate}
\end{propp}
\begin{note}
  The map $(x, y) \mapsto \langle x, y\rangle$ is continuous on $X\times X$. 
\end{note}
For each $y\in X$, define 
\[(Jy)(x) = \langle y, x \rangle.\]
$Jy : X \to \mathbb F $ is clearly linear, and by CS, 
\[|(Jy)(x)|\leq \|y\|\|x\|\]
in fact, 
\[|(Jy)(y)|= \|y\|\|y\|\]
So, $Jy \in X'$. Consequently, $J: X\to X'$ is bounded, antilinear, and one-to-one. We will see later that it is also onto.  

\begin{propp}[Parallelogram Law]
  \[\|x+y\|^2 + \|x-y\|^2 = 2\|x\|^2 + 2\|y\|^2 .\]
\end{propp}
\begin{proof}
  Computation exercise. Just write it out. 
  \begin{align*}
    {\left\| {x + y} \right\|^2} + {\left\| {x - y} \right\|^2} &= \left\langle {x + y,x + y} \right\rangle  + \left\langle {x - y,x - y} \right\rangle  \hfill \\
     &= \left\langle {x,x} \right\rangle  + \left\langle {y,y} \right\rangle  + 2Re\left\langle {x,y} \right\rangle  + \left\langle {x,x} \right\rangle  + \left\langle {y,y} \right\rangle  - 2Re\left\langle {x,y} \right\rangle  \hfill \\
     &= 2{\left\| x \right\|^2} + 2{\left\| y \right\|^2} 
  \end{align*}
\end{proof}
\begin{thm}[Best Approximation]
  Let $X$ be Hilbert. Assume $U \subset X$ is nonempty, convex, and closed. Then given any $x\in X$, there exists a unique $u_0 \in U$ such that $d(x,U) = \|x-u_0\|$. 
\end{thm}
\begin{proof}
  See notes for a picture, uses parallelogram law. 

  Pick a sequence $n\mapsto u_n$ such that $\|x-u_n\|=:d_n \to d = d(x,U)$. Then 
  \[{\left\| {{u_n} - {u_m}} \right\|^2} = 2{\left\| {{u_m}} \right\|^2} + 2{\left\| {{u_n}} \right\|^2} - {\left\| {\left( {x - {u_n}} \right) + \left( {x - {u_m}} \right)} \right\|^2}.\]
Now rewrite \[{\left\| {\left( {x - {u_n}} \right) + \left( {x - {u_m}} \right)} \right\|^2} = 4{\left\| {x - \frac{{{u_n} + {u_m}}}{2}} \right\|^2} \geq 4{d^2}\]
so we have
\[{\left\| {{u_n} - {u_m}} \right\|^2} \leq 2{\left\| {{u_m}} \right\|^2} + 2{\left\| {{u_n}} \right\|^2} - 4{d^2} \to 0.\]
So, the sequence $n\mapsto u_n$ is Cauchy and converges since we are in a Hilbert Space. 
\end{proof}
\begin{definition}
  For $U \subset X$, define the \underline{orthogonal complement} of $U$, denoted $U^\perp$, as 
  \[U^\perp = \{x\in X : \langle u,x\rangle=0 \,\, \forall u \in U \}.\] Beware of the notational conflict with the previous definition...when we show that $X$ is isometric with $X'$, this issue goes away. 
\end{definition}

\begin{propp}
  Let $X$ be an inner product space and $U\subset X$ a subspace. 
  If $x\in X$ and $u_0 \in U$, then 
  \[\left\| {x - {u_0}} \right\| = d\left( {x,U} \right) \Leftrightarrow x - {u_0} \in {U^ \bot }.\]

\end{propp}
\begin{proof}
  $(\Leftarrow)$ is trivial. Use Pythagoras.

  $(\Rightarrow)$. Pick any $u \in U$. Pick $\lambda \in \C$ with $|\lambda | =1$ so that $|\langle x-u_0, \lambda u\rangle| = |\langle x-u_0, u\rangle |$. Then for every $t\geq 0$, we have 
  \[0 \leqslant {\left\| {\left( {x - {u_0}} \right) - t\lambda u} \right\|^2} - {\left\| {x - {u_0}} \right\|^2} =  - 2t\left| {\left\langle {x - {u_0},u} \right\rangle } \right| + {t^2}{\left\| u \right\|^2}\]
  which implies that \[2\left| {\left\langle {x - {u_0},u} \right\rangle } \right| \leq t{\left\| u \right\|^2}\] which holds for all $t>0$, so it must be that $\langle x-u_0, u\rangle =0$. 
\end{proof}
\begin{definition}
  Let $U$, $V$ be subspaces of $X$. We say that $U$ and $V$ are orthogonal, denoted $U\perp V$, iff 
  \[\langle u, v\rangle = 0\quad \forall u \in U, v\in V.\]
\end{definition}
\begin{definition}
  A projection $P:X\to X$ is called orthogonal iff $\mathcal R(P) \perp \mathcal N(P)$. 
\end{definition}
\begin{ex}
  Let $\{x_1, x_2, \dots, x_n\}$ be a mutually orthogonal set with $\|x_j\|=1$. Notice that this implies the set is linearly independent, since \[\|c_1x_1 + \dots + c_n x_n\|^2 = \langle c_1x_1 + \dots + c_nx_n,c_1x_1 + \dots + c_nx_n \rangle = \sum c_k^2.\]
  Define a projection $P: X\to X$ by setting $Px = \sum\limits_{j=1}^n \langle x_j, x\rangle x_j$. To see that $P^2=P$, write $x = \sum c_ix_i$. Clearly, $P$ is also linear. Finally, $\mathcal R(P) = span\{x_1,\dots,x_n\}$ and $\mathcal N(P) = \mathcal R(P)^\perp$. 
\end{ex}


\subsection*{adf}
\subsection*{asdf}
\subsection{Spectral Theory II}


\cred 11/7 Lecture...\cbk

\begin{prop}
  Let $X$ be Hilbert over $\C$. \cg Actually, we may not need Hilbert, just some IPS may work... \cbk Assume that $\langle x, Ax \rangle = 0 $ for every $x\in X$. Then $A=0$. \cg Notice that if the statement had read $\langle y, Ax \rangle = 0$ for every $x, y\in X$, then the statement is obvious. Pick $y = Ax$, then you get $\|Ax\|=0$ for every $x$ and so $A=0$. This direction is not as trivial, but we will take inspiration from the reverse direction in this proof. \cbk
\end{prop}
\begin{proof}
  For every $x, y\in X$, we have
  \begin{align*}
    0 &= \left\langle {x + y,A\left( {x + y} \right)} \right\rangle  \hfill \\
     &= \left\langle {x,Ax} \right\rangle  + \left\langle {y,Ay} \right\rangle  + \left\langle {x,Ay} \right\rangle  + \left\langle {y,Ax} \right\rangle  \hfill \\
     &= \left\langle {x,Ay} \right\rangle  + \left\langle {y,Ax} \right\rangle  
  \end{align*}
  \cg Recall inner prod is antilinear in the first argument. \cbk 
  Since the statement holds for every $y\in X$, we can, in particular, consider applying the statement to the vector $iy$. In this case, we find 
  \[0 = i\langle x, Ay\rangle -i \langle y, Ax\rangle.\]
  \cg again, note the complex conjugate.\cbk If we combine these equations, we see that $-2i\langle y, Ax\rangle =0$ which implies $\langle y, Ax\rangle = 0 $ for every $y, x$. We choose $y = Ax$ and then conclude the result. 
\end{proof}
\begin{note}
  The statement is false in general. In dimension 1, for example, it is always true. However, for $X$ over the reals (say, $\R^2$), if we consider the linear operator given by a rotation by $90^\circ$, certainly the statement is false. \cg by definition, $Ax$ would be orthogonal to $x$ for every $x\in X$, but $A$ is most definitely nonzero.\cbk 
\end{note}
\begin{corollary}
  Let $X$ be an inner product space over $\C$. Let $A\in L(X)$. If $\langle x, Ax\rangle$ is real for every $x\in X$, then $A= A^*$. 
  \cg The converse we know already, in fact. \cbk
\end{corollary}
\begin{proof}
  If $\langle x, Ax\rangle \in \R$, then 
  \[\langle x, Ax\rangle = \langle Ax, x \rangle = \langle x, A^* x \rangle\] for every $x\in X$. In particular, this means that 
  \[\langle x, (A-A^*)x\rangle = 0 \] for every $x$. By the previous proposition then, $A = A^*$. 
\end{proof}

We want to introduce the notion of the \textit{absolute value} of an operator, something along the lines of what we have for complex numbers, i.e. $|z| = \sqrt{\bar z z }$. What we will end up defining is that $|A| := \sqrt{A^* A }$. But first, we need to set up some things. 
\begin{lemma}
  Assume that $A$ is bounded, nonnegative, i.e. $A\in L(X)$ and $A^* = A \geq 0$. Then 
  \begin{enumerate}[label=(\alph*)]
    \item $A^n\geq 0$ for all $n$, 
    \item If $A\leq a I$, then $\|A\|\leq a$, 
    \item $|\langle y, Ax\rangle | \leq \sqrt{\langle y, Ay\rangle \langle x, Ax\rangle }$
  \end{enumerate}
\end{lemma}
\begin{proof}
  \begin{enumerate}[label=(\alph*)]
    \item Address in two cases, if $n$ is even, then
    \[\left\langle {x,{A^{2m}}x} \right\rangle  = \left\langle {{A^m}x,{A^m}x} \right\rangle  \geqslant 0\] 
    If $n$ is odd, then 
    \[\left\langle {x,{A^{2m + 1}}x} \right\rangle  = \left\langle {{A^m}x,A{A^m}x} \right\rangle  = :\left\langle {y,Ay} \right\rangle  \geqslant 0.\]
    \item Well, given this inequality, we just sandwich the guys between $x$ and conclude the result. Observe that if $A\leq aI$, then $\langle x, Ax\rangle \leq a \langle x, x\rangle$ from which we conclude $\|A\| \leq a$. 
    \item Consider the alternate inner product 
    \[\langle y, x\rangle ' := \langle y, Ax \rangle \]
    \cg Note that this fails to be an inner product for $x\in \mathcal N(A)$, since it may be zero when $x\neq 0$. However, the proof follows the same proof as now the regular Cauchy Schwarz inequality. 
    \end{enumerate}
\end{proof}
\begin{definition}
  In $L(X)$, pointwise converge is also called \underline{strong convergence}. That is, 
  \[\slim\limits_{n \to \infty } {A_n} = A\quad  \Longleftrightarrow \quad \mathop {\lim }\limits_{n \to \infty } {A_n}x = Ax\]
\end{definition}
Notice in particular that this is different from \textit{norm convergence}, where $\|A_n - A\|\to 0$. Strong convergence is slightly weaker than norm convergence. 
\begin{prop}
  $X$ Hilbert, Assume that $0\leq A_1 \leq A_2 \leq \dots \leq aI$ with $A_j^* = A_j \in L(X)$. Then $A= \slim_n A_n$ exists, $A= A^*$, and $0\leq A \leq aI$. 
\end{prop}
\begin{proof}
  Let $n>m$. \cg Our goal here is to show that the sequence $n\mapsto A_n x$ is a Cauchy sequence. We can then exploit completeness. \cbk Notice that since the sequence of $A_n$ is increasing, then $0 \leq A_n - A_m \leq aI$ and 
  \[{\left\| {{A_n}x - {A_m}x} \right\|^2} = \left\langle {\left( {{A_n} - {A_m}} \right)x,\left[ {{A_n} - {A_m}} \right]x} \right\rangle .\]
\end{proof}

\cred finish 11/7 lecture notes \cbk 

\cblu (11/9 lecture) \cbk 

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\begin{theorem}[Square Root]
  Let $T\in L(X)$ for a Hilbert space $X$ such that $T^*=T$ and $T \geq 0$. \cg i.e. $\langle x, Tx\rangle \geq 0$ for every $x\in X$.\cbk  Then there exists a unique $S\in L(X)$ such that $S^* = S$, $S\geq 0$, and $S^2 = T$. Additionally, if $U\in L(X)$ commutes with $T$, then $U$ commutes with $S$ and we write $S = T^{1/2}$. 
\end{theorem}
\end{tcolorbox}

\begin{definition}
  Define for $A\in L(X)$ its \underline{absolute value}, denoted $|A|$, where $|A| = \sqrt{A^*A}$. \cg akin to how we do for complex numbers, where $|z| =\sqrt{\bar z z }$. \cbk Note that if $A$ is normal, then $|A|$ commutes with $A$ and $A^*$ \cred why? \cbk which follows from previous proof.   
\end{definition}
The notion of the square root combined with the absolute value allows us to arive at the following theorem: 

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\begin{theorem}[Polar Decomposition]
  If $A\in L(X)$, then there exists a partial isometry $U\in L(X)$ such that $A = U|A|$. \cg we will specify in a moment what exactly is meant by a partial isometry. \cbk In addition, if $A$ is normal, then we can choose a unitary $U$ that commutes with $A$. 
\end{theorem}
\end{tcolorbox}
\begin{proof}
  
\end{proof}








\subsection{Sturm-Liouville Theory}














\cred
11/14 Lecture last theorem in SL problems, \cbk 
\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\begin{theorem}
  Assume that $u$, $v$ satisfy 
  \begin{enumerate}
    \item $Au = 0,\quad \alpha_1 u(0) + \alpha_2 u'(0) =0, $
    \item $Av=0, \quad \beta_1 v(1) + \beta_2 v'(1) = 0$. 
  \end{enumerate}
  Assume that 0 is not an eigenvalue of $A$, so $u\neq v$. \cg why? If $u$ were equal to $v$, then \cbk Then 
  \[G\left( {s,t} \right) = \left\{ {\begin{array}{*{20}{c}}
    {cu(s)v(t)}&{s\leq t} \\[.25cm] 
    {cu(t)v(s)}&{t \leq s} 
  \end{array}} \right.\]
  for some $c\neq 0$. \cg Remark: $G$ here is the Green's function of the operator $A$. We can determine exactly what is $c$, but not now...\cbk 
\end{theorem}
\end{tcolorbox}


\newpage \section{Distributions}
\subsection{Test Functions and Distributions}
Motivation: we motivate the use of distributions as we sometimes wish to generalize notions in PDE's to the largest group of functions possible. \cg Mentioned in class: see the middle third function problem from measure theory, this was actually very interesting! ok could not find online, but you take $f(x) = x$ on $(0,1)$ and break into thirds. Break each into thirds again, set the middle interval flat. Repeat for each sub interval. At the end of the day you are left with a function that has derivative zero everywhere that actually is equal to a function with derivative 1. \cbk 

Let $\Omega \subset \R^d$ open. 
\begin{definition}
  A test function $f$ is $f\in C^\infty(\Omega)$ with compact support. We denote this space $\mathcal D (\Omega)$. 
\end{definition}
A natural question to ask is: \textit{what is an example of such a function?} Indeed, it is perhaps somewhat difficult to convince yourself that such nice functions exist. Analytically, this is impossible \cg think, any polynomial, for example, seems like a reasonable candidate, but a polynomial on its own has only finitely many zeros, so it is out the window. \cbk To that end, we have the following example: 
\begin{ex}
  Simple example, take $e^{1/(x^2-1)}$ for $|x|<1$, and zero elsewhere. This looks like a bump, hence why test functions in this space commonly take the nickname bump functions. Notice the following properties: 
  \[\lim_{x\to 1^-}e^{1/(x^2-1)} = \lim_{x\to -1^+}e^{1/(x^2-1)}= 0,\] 
  and
  \[\lim_{x\to 1^-}D^\alpha e^{1/(x^2-1)} = \lim_{x\to -1^+}D^\alpha e^{1/(x^2-1)}= 0.\]
  So, the support is contained in the compact interval $[-1,1]$ and the function is infinitely continuously differentiable. This is our first example of a test function. 
\end{ex}
Each $f\in \mathcal D(\Omega)$ belongs to one of $\mathcal D (K)$, where for $K\subset \Omega$ compact, we define 
\[\mathcal D (K) = \mathcal D_K(\Omega) = \left\{ f\in \mathcal D(\Omega) : \text{supp}\, f \subset K\right\}\]
and equip $\mathcal D_K$ with the (semi) norms 
\[\|f\|_N = \max_{|\alpha|\leq N} \max_{x\in K} \left\vert D^\alpha f(x) \right\vert.\]
\cg Are $\|\cdot \|_N$ norms or seminorms? The only one thing to check is whether or not $f =0$ when $\|f\|_N = 0$. Indeed, this does appear to be the case. A quick proof by contradiction: suppose to the contrary that $f \neq 0$, but $\|f\|_N = 0$. Then we see that for $\alpha = 0$, $\|f\|_N$ is nonzero, a contradiction. So technically speaking, $\|\cdot\|_N$ is a full norm. \cblu However, just remember that $\mathcal D_K$ is still a seminormed space, since the topology on it is defined in terms of infinitely many norms. Also, some people use $f \mapsto \|f\|_\alpha = \max_{x\in K} \vert D^\alpha f (x) \vert $ as the seminorms for each $\alpha$. In this case, $\|f\|_\alpha$ is not a norm, but the resulting topology is the same. \cg Last comment on this: every norm is a seminorm, so to use the term seminorm is not incorrect, especially if we do not plan to exploit the defining condition that makes it a full norm. \cbk
We shall see that it is much easier to deal with spaces $\mathcal D_K$ than it is to deal with $\mathcal D(\Omega)$. \cg One immediate reason this is the case is because the spaces $\mathcal D_K$ are metrizable as they are equipped with a countable family of norms. \cbk 
\begin{note}
  Notice the following additional properties: 
\begin{itemize}
  \item If $f\in \mathcal D_K$, then $D^\alpha f \in \mathcal D_K$ for every $\alpha$. \cg $f$ has as many derivatives as you want it to, and their support will not increase. Notice in particular that if $f \in \mathcal D_K$, then where $f$ vanishes, its derivatives must also vanish. \cbk 
  \item The sequence of norms is increasing \cg(since we are taking supremum now over a larger set)\cbk. That is, 
  \[\|f\|_1 \leq \|f\|_2 \leq \|f\|_3\leq \dots \]
  \item $\mathcal D_K$ is metrizable. \cg and most importantly, $\mathcal D(\Omega)$ is not!\cbk 
  \item Every open set $U\ni 0$ includes a set 
  \[U_{N,r}(f) = \left\{ g\in \mathcal D_K : \|g-f\|_N < r\right\}.\]
  \cg This is a consequence of Baire. If we think about what this statement is really saying, it says that any open set containing zero has a nonempty interior. (i.e. there is at least one interior point in $U$, namely $f$). By definition, this means that we can fit an open ball that is centered at $f$ completely inside $U$. \cbk 
\end{itemize}
\end{note}
\begin{note}
  With this topology, the operation of translation, i.e. $f \mapsto f + f_0$ is continuous. \cg I think this follows from the last bullet of the previous note. Write out the definition of continuity and apply equivalently the condition that the map is continuous at zero.\cbk 
\end{note}
\begin{prop}[Convergence in $\mathcal D_K$]
  Let $f, f_1, f_2, \dots \in \mathcal D_K$. Then $f_n \to f$ in $\mathcal D_K$ if and only if $D^\alpha f_n \to D^\alpha f$ in $C_0(K)$ for every $\alpha$. \cg That is to say, convergence of $f_n$ in the space $\mathcal D_K$ implies, in particular, \cred uniform convergence \cg of all of $f$'s derivatives (since the norm on $C_0(K)$ is the sup-norm).\cbk 
\end{prop}
\begin{proof}
  Follows from the definition! \cg Notice that if $f_n$ converges in the norm, we must have that all derivatives go uniformly!\cbk 
\end{proof}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\begin{theorem}[Characterization of Distributions]
  A continuous linear functional $T \in \mathcal D_K'$ \cg aka $T$ is continuous, \cbk  if and only if there exists $N\geq 0, c>0$ such that 
  \[|T(f)|\leq c \|f\|_N\]
  for every $f \in \mathcal D_K$. 
\end{theorem}
\end{tcolorbox}
\begin{proof}
  Assume there exists such an $N$ and $c$. Then $T(f_n) \to 0$ whenever $f_n \to 0$, which is to say, $T$ is sequentially continuous. But, since $\mathcal D_K$ is metrizable, sequential continuity is equivalent to continuity, so $T$ is continuous at zero, and thus continuous.
  
  On the other hand, assume now that $T:\mathcal D_K \to \mathbb F$ is continuous. Then $T$ is in particular continuous at zero. Then the set 
  \[U = \{f \in \mathcal D_K : |T(f)|<1 \}\] is an open set and contains $f = 0$. \cg Notice, $U$ is nothing more than the inverse image of the unit ball in $\mathbb F$. \cbk  By our previous note, we conclude that there is a set (i.e. there exists an $N$, $r$) 
  \[V = \{g \in \mathcal D_K : ||g||_N<r\}\]
  that is fully contained in $U$. If now $\|f\|_N = r/2$, say, then 
  \[|T(f)| \leq  1 = \frac{2}{r}\|f\|_N.\]
  Now by linearity, the argument extends to any $f$. Just scale $f$ appropriately and get it back in the ball of radius $r/2$. 
  \cg Notice this proof is just saying that a linear functional is continuous if and only if it is bounded! We have seen this proof before. \cbk 
\end{proof}
We have now the following lemma 
\begin{lemm}
  Let $1 \leq j \leq d$. Let $n\mapsto f_n$ and $n\mapsto F_n$ be converging sequences in $C_0(\R^d)$ with $F_n \to F$ and $f_n \to f$ uniformly. If $f_n = \partial_{x_j} F_n$ for all $n$, then $f = \partial_{x_j} F$. \cg i.e. the limit of the derivatives is the derivative of the limit. \cbk 
\end{lemm}
\begin{proof}
  May assume $\mathbb F = \R$. In either case, we can simply handle real and imaginary parts separately. Take also $d = 1$. Let $x\neq y$. Because $f_n\to f$ and $F_n \to F$ in the sup norm, then there exists an $N$ such that for any $n > N$,  
  \[|F(t) - F_n(t) |\leq |x-y|^2 \]
  and 
  \[|f(t) - f_n(t) |\leq |x-y|. \]
  \cg Just a consequence of convergence in $C_0$, nothing to it. You can make the differences on the left as small as you wish. In particular, smaller than the choices we take. Note that at this moment, $t$ is any number in $\R$. \cbk Now we choose $t$ such that 
  \[F_n(x) - F_n(y) = (x-y) f_n(t),\qquad \text{(Mean Value Theorem)} \]
  \cg Note, we are not saying that the two inequalities following from convergence imply the mean value theorem, we are just applying the mean value theorem. \cbk 
  Then with triangle inequality, 
  \begin{align*}
    |F(x) - F(y) - (x-y) f(t)| &\leq |F(x) - F_n(x) +F_n(y) - F(y) +F_n(x) - F_n(y) - (x-y) f(t)| \\
    &\leq |F(x) - F_n(x)| + |F_n(y) - F(y)| + |x-y||f_n(t) - f(t)|\\
    &\leq |x-y|^2 + |x-y|^2 + |x-y|^2\\
    &\to 0 \text{ as } x\to y.
  \end{align*}
  Thus we conclude that $f = \partial_{x_j}F$ as desired. 
\end{proof}
A natural issue that now arises is how we should define the notions of convergence, Cauchy-ness, and completeness in \textit{seminormed} spaces $\mathcal D_K$. 
\begin{definition}
  Let $(V,\mathcal R)$ be a seminormed space. \cg Here, $\mathcal R$ is just a family of seminorms, i.e. $\mathcal R = \{\rho_1, \rho_2, \dots\}$. \cbk We say that a sequence $n \mapsto v_n \in V$ is Cauchy if and only if 
  \[\rho(v_n - v_m) \to 0 \quad \text{ as } \quad n, m\to \infty\]
  for every $\rho \in \mathcal R$. \cg Contrast this with our usualy definition of Cauchy...\cbk 
  
  We say that $(V, \mathcal R)$ is complete if and only if every Cauchy sequence converges to a limit in $V$. 
\end{definition}

We now apply this lemma to $\mathcal D_K$. 
\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\begin{theorem}
  $\mathcal D_K$ is complete. 
\end{theorem}
\end{tcolorbox}
\begin{proof}
  Consider a Cauchy sequence $n\mapsto f_n \in \mathcal D_K$. Then, $n \mapsto D^\alpha f$ is a Cauchy sequence in $C_0(K)$. Since $C_0$ is complete, then $D^\alpha f$ converges to $f_\alpha \in C_0$, say, uniformly. Apply now the lemma to conclude that $f_\alpha = D^\alpha f_0$, so $f_n$ has a limit and ..\cred not sure of the remaining details, although the end statement is of course that $f_n \to f$ and $f \in \mathcal D_K$. \cbk 
\end{proof}
\begin{definition}
  $\mathcal D(\Omega)$ is the space of all $C^\infty$ functions on $\Omega$ with compact support in $\Omega$. Notice that $\mathcal D(\Omega) = \bigcup_{K\subset \Omega } \mathcal D_K$. Furthermore, $\mathcal D(\Omega)$ is equipped with the set $\mathcal R$ of seminorms satisfying 
  \[\rho \in \mathcal R \quad \Longleftrightarrow \quad  \rho|_{\mathcal D_K} \text{ is continuous for all compact } K \subset \Omega.\]
\end{definition}
Notice that all of the norms we defined earlier, $\|\cdot \|_N$, belong to $\mathcal R$. \cg What else is in $\mathcal R$? don't know at the moment, but in principle there could be more? \cbk 
The definitions actually rise very nautrally, as we shall see. In practice, we will often relate everything we do in $\mathcal D(\Omega)$ back to something in one of the $\mathcal D_K$ and then work from there. 

\begin{lemm}
  Consider a sequence $n \mapsto f_n \in \mathcal D(\Omega)$. Then either there exists a compact $K \subset \Omega$ such that $f_n \in \mathcal D_K $ for every $n$, or else there exists a $\rho \in \mathcal R$ such that $n \mapsto \rho(f_n)$ is unbounded. 
\end{lemm}
\begin{proof}
  Let $S = \bigcup_n \text{supp } f_n$. If $\overline S$ is compact and fully contained in $\Omega$, then we are done. That is the easy case. 
  Otherwise, our task is to construct a $\rho \in \mathcal R$ such that $n \mapsto \rho(f_n)$ is unbounded. We select a subsequence $j\mapsto g_j = f_{n_j}$ and a sequence of points $j \mapsto x_j$ such that $g_j(x_j) \neq 0$. This implies that either $|x_j|\to \infty$ or $x_j$ goes to the boundary, $\R^n -S$. \cred why? explain. \cbk Define the seminorm 
  \[\rho(f) = \sum_j \left\vert \frac{j}{g_j(x_j)} f(x_j) \right\vert \]
  Given a compact $K\subset \Omega$, the set $\{j : x_j \in K\}$ is finite, so $\rho \in \mathcal R$. Additionally, evaluation is continuous in $\mathcal D_K$, so $\rho \in \mathcal R$ \cg why? well, the definition states that $\rho\in \mathcal R$ iff its restriction to $\mathcal D_K$ is continuous for any compact $K\subset \Omega$. We see that when restricted to a compact set, there are only finitely many terms in the sum (since there are only finitely many $x_j$, so we are left with a finite sum that involves evaluations, which is all continuous. ) \cbk 
  
  However, $\rho(g_j) \geq j$, so it blows up. 
\end{proof}



\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\begin{theorem}
  $\mathcal D(\Omega)$ is complete, and the following are equivalent:
  \begin{enumerate}[label=(\alph*)]
    \item $n\mapsto f_n$ converges in $\mathcal D(\Omega)$, 
    \item $n \mapsto f_n$ is Cauchy in $\mathcal D(\Omega)$, 
    \item $n \mapsto f_n$ is Cauchy in $\mathcal D_K$ for some compact $K\subset \Omega$. 
  \end{enumerate}
\end{theorem}
\end{tcolorbox}

\begin{proof}
  $(a) \implies (b)$ Obvious. 

  $(b) \implies (c)$ Choose $K$ compact and consider $n \mapsto f_n \in \mathcal D_K$. Our task is to show that $f_n$ is Cauchy in $\mathcal D_K$. Assume that $f_n$ is Cauchy first in $\mathcal D(\Omega)$, i.e. $\rho(f_n - f_m) \to 0$ as $n, m\to \infty$ for every $\rho \in \mathcal R$. Pick $\rho = \|\cdot\|_N$. So then we have a Cauchy sequence in $\mathcal D_K$. \cred some explanation needed here...\cbk 

  $(c) \implies (a)$ Consider a compact $K$ and a Cauchy sequence $n\mapsto f_n \in \mathcal D_K$. Then by completeness, $f_n \to f$ in $\mathcal D_K$. (What remains now is to demonstrate convergence in $\mathcal D$) However, if $\rho \in \mathcal R$, then $\rho(f_n -f) \to 0$ since $\rho|_{\mathcal D_K}$ is continuous. 
\end{proof}


\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\begin{theorem}
Let $X$ be any seminormed space, and let $A: \mathcal D(\Omega) \to X$ be linear. Then the following are equivalent: 
\begin{enumerate}[label=(\alph*)]
  \item $A$ is continuous, 
  \item If $f_n \to 0$ in $\mathcal D(\Omega)$, then $Af_n \to 0$ in $X$, \cg notice we are not in a metric space...so it is interesting that we have equivalence of sequential continuity and continuity itself!\cbk 
  \item $A|_{\mathcal D_K}$ is continuous for all compact $K\subset \Omega$. 
\end{enumerate}
\end{theorem}
\end{tcolorbox}

\begin{proof}
  $(a) \implies (b)$ Immediate. Continuity always implies sequential continuity.

  $(b) \implies (c)$ Fix $K$ and look at $\mathcal D_K$. How to pick $K$? By the previous theorem, we know that if $f_n \to 0$ in $\mathcal D(\Omega)$, then $f_n$ is Cauchy in $\mathcal D_K$. By assumption, $Af_n \to 0$ in $X$. Since $\mathcal D_K $ is metrizable, then $A$ is continuous at zero, but by linearity, then $A$ is continuous everywhere. \cred anything more to say here? \cbk 

  $(c) \implies (a)$ Let $U\ni 0$, $U\subset X$ be open. Then $U$ contains 
  \[V = \{x\in X : \rho_1(x) < r, \dots, \rho_m(x) < r\}\]
  for some $\rho_1, \dots, \rho_m$ seminorms on $X$ and some $r>0$. Then 
  \[A^{-1} V = \{f\in \mathcal D(\Omega) : \rho_1(Af) <r, \dots, \rho_m(Af)<r \}\]
  is open since $\rho_j\circ A$ are continuous when restricted to any $K$. So, $\rho_j \circ A \in \mathcal R$, and so $A^{-1}V$ is open. ....\cred what else now? End of proof? \cbk 

\end{proof}



\subsection{Calculus with Distributions}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\begin{theorem}
The derivative operator 
\[D^\alpha : \mathcal D (\Omega) \to D (\Omega)\] is continuous. 
\end{theorem}
\end{tcolorbox}
\begin{proof}
  Use the fact that 
  \[\|D^\alpha f\|_N \leq \|f\|_{N + |\alpha|}\]
  for every compact $K \subset \Omega$, $f\in \mathcal D_K$. This tells us that the restriction of $D^\alpha$ to any $\mathcal D_K$ is continuous, and therefore $D^\alpha$ is continuous on the whole space. 
\end{proof}
\begin{definition}
  For $T \in \DD'(\Omega)$, we define 
  \[D^\alpha T := (-1)^{|\alpha|} T\circ D^\alpha\]
  \cg Remark: This is basically using integration by parts to define the derivative of a distribution. In retrospect, a very natural definition since $f$ is a very nice function. More explicitly, this is definition says that for any $f \in \DD (\Omega)$, $(D^\alpha T) (f) = (-1)^{|\alpha|} T(D^\alpha f)$. \cbk
\end{definition}
\begin{ex}
  Let $\Omega = \R$. 
  \begin{enumerate}[label=(\alph*)]
    \item Let $h(x) = \frac{1}{2}{sgn} (x)$. Then $h' = \delta_0$ in the sense that if $H(f)$ is the distribution generated by $h(x)$, i.e. $f \ni \DD \to H(f) = \int hf \in \R$, then $H'(f) = \delta_0 (f)$. Why is this the case? 
    By our definition, 
    \[H'(f) = -\int h(x) f'(x) dx\]
    and more specifically, 
    \[H'f =  \frac{1}{2} \int\limits_{-\infty}^0 f' - \frac{1}{2}\int\limits_{0}^{\infty} f' = \int\limits_{-\infty}^0 f' = f(0).\]
    \cg Keep in mind that $f$ has compact support, so it has to vanish eventually, so $f(-\infty) = 0$. \cbk
    \item Let $g(x) = \frac{1}{2}|x|$. Then $g' = h$ also in the distributional sense. Furthermore, $g'' = \delta_0$. 
    \item $\displaystyle \frac{d}{dx}\log |x| = \mathcal P \left(\frac{1}{x}\right)$ in the sense that 
    \[ - \int {\log \left( x \right)f'\left( x \right)dx}  = \mathop {\lim }\limits_{\varepsilon  \to 0} \int\limits_{\left| x \right| \geqslant \varepsilon } {\frac{1}{x}f\left( x \right)dx} \]
    \cg little fact: $\lim \varepsilon \ln \varepsilon = 0$.\cbk 
  \end{enumerate}
\end{ex}



\begin{lemma}
  Let $g \in C^\infty (\Omega)$. Then 
  \[f \mapsto gf\] is continuous on $\DD(\Omega)$. \cg (Multiplication by a $C^\infty$ function is a continuous operation.)\cbk 
\end{lemma}
\begin{proof}
  \cg First a reminder on what we need to show - to show that a linear operation is continuous it is enough to show that it is bounded. To that end, we want to get a bound on $|D^\alpha gf|$, since norms in $\DD$ also look at derivatives.\cbk To get the general result, we apply Leibnitz's Rule: 
  \[D^\alpha(gf) = \sum_{\beta + \gamma = \alpha} \binom{\beta}{\alpha} D^\beta g D^\gamma f \] where 
  \[\binom{\beta}{\alpha} = \prod_j \binom{\beta_j}{\alpha_j}  = \prod_j \frac{\alpha_j!}{(\alpha_j - \beta_j)! \beta_j!}.\]
  And so 
  \[|D^\alpha gf |\leq C\|g\|_{|\alpha|}\|f\|_{|\alpha|}. \]
  \cg I don't really know how to conclude this, but it has something to do with the fact that the $|\alpha|$ norm has the sup over all derivatives of total order $\alpha$, and so one can claim the bound. \cbk 
\end{proof}
\begin{definition}
  For $g \in C^\infty (\Omega)$ and $T \in \DD' (\Omega)$, we define the distribution $gT$ by 
  \[(gT) (f) = T(gf).\]\cg nothing fancy here, just a very natural way to define multiplication by a bounded function. \cbk
\end{definition}
\begin{ex}
  Let $x \in \Omega = \R$. Let us explore the distribution $g\delta_x'$. Applying our rules so far, 
  \[(g\delta_x')(f) = \delta_x'(gf) = \delta(x) (gf)' = g(x) f'(x) + g'(x) f(x).\]
  But, we would not like to keep any derivatives on $f$ if we want to write the expression more generally and not pointwise, so we say that 
  \[g\delta_x' =g'(x) \delta_x -g(x) \delta_x'.\]
\end{ex}

\begin{lemma}
  For any $g \in C^\infty(\Omega)$ and $T \in \DD'(\Omega)$, 
  \[D^\alpha(gT) = \sum_{\beta+\gamma=\alpha} \binom{\beta}{\alpha} (D^\beta g)(D^\gamma T).\]
\end{lemma}
\begin{proof}
  Omitted for now...
\end{proof}
\begin{example}
  $V'' + V = \delta_0$ for $v = \frac{1}{2}\sin |x|$.  
  \cg in some sense, although we have not introduced it yet, $v$ here is the fundamental solution (or Green's Function) of the differential equation $v'' + v = f$.\cbk 
  In any case, first write $v = gh$ where $g(x) = \sin x$ and $h(x) = \frac{1}{2}sgn(x)$. Then let us check that indeed, $(gH)'' + gH = \delta_0$. Notice first that $g'' = -g$ and as we showed earlier, $H' = \delta_0$. As a result, 
  \[(gH)'' + gH = gH'' + 2g'H' + g''H + gH = gH'' + 2g'H' .\]
  Looking at each term, 
  \[\left( g'H' \right) = H'\left( g'f \right) = \delta_0 \left( g'f \right) =g'(0) f(0) = f(0),\]
  and 
  \[ \left( gH'' \right) (f) = H''(gf) = -H'\left( \left( gf \right)' \right) = -\delta_0(gf' + g'f) = -f(0).\]
  And so $\left( gH \right)'' + gH = -\delta_0 + 2\delta_0 = \delta_0$. 
  \end{example}

  We will now discuss another important operation on distributions: the operation of \textit{convolution}. 

  For $f \in \DD(\R^n)$ and $ y \in \R^n$, define the operator 
  \[\left( \mathcal{T}_xf \right)(y) = f(y-x)\]
  and define $\hat{f}(x) = f(-x)$ for $x \in \R^n$. If $h \in L^1_{loc}(\R^n)$, define the convolution 
  \[\left( h\ast f \right) (x) = \int h(y) f(x-y) dy.\]
  Notice first that $f(x-y) = \hat{f}(y-x)$, and $\hat f(y-x) =\left( \mathcal T_x \hat f \right)(y)$, so in essence, 
  \[\left( h\ast f  \right) (x) = \int h(y) \left( \mathcal T_x \hat f \right)  (y) dy = H\left( \mathcal T_x \hat f \right) (x) \]
  where $H$ is the distribution generated by $h$. 

  \begin{definition}
    For $f \in \DD(\R^n)$ and $T \in \DD'(\R^n)$, define 
    \[\left( T\ast f  \right)(x) = T\left( \mathcal T_x \hat f \right).\]
    \cg We are defining here the notion of the convolution of a distribution with a test function. Remark: do we need the $(x)$ on the RHS? left side is pointwise and right side has no argument...\cbk 
  \end{definition}

  \begin{example}
    Let us take a look at our favorite distribution, $\delta_0$. For an $f \in \DD$, 
    \[\left( \delta_0 \ast f \right)(x) = \delta_0\left( \mathcal T_x \hat f \right)  = \left( \mathcal T_x \hat f \right)(0) = \hat f (-x) = f(x) .\]
    We conclude then that $\delta_0 \ast f = f$. That is, the delta distribution is an identity under convolution. 
  \end{example}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\begin{theorem}
  Let $T \in \DD'(\R^n)$, $f \in \DD(\R^n)$. Then the convolution $T\ast f$ is smooth, and 
  \[D^\alpha\left( T\ast f  \right) = \left( D^\alpha T \right)\ast f = T\ast \left( D^\alpha f \right)\] for every $\alpha$. 
\end{theorem}
\end{tcolorbox}

\begin{proof}
  ...The second euqality is by definition. The challenge is to show that the very left is equal to the very right. An outline of the proof: 
  First replace $D^\alpha$ with a translation and show that the translation satisfies the equality. Then, we use our lemma that the finite difference approximation of the derivative indeed converges to the derivative on $\DD'$. For $|\alpha| = 1$, for instance $D^\alpha_\varepsilon = \frac{1}{\varepsilon}\left[ \mathcal T_0 - \mathcal T_{\varepsilon\alpha} \right] $. 
\end{proof}

\begin{example}
  For every $f \in \DD(\R)$, 
  \[\left( \frac{d^2}{dx^2}+ I  \right) \int \frac{1}{2}\sin|y| f(x-y) dy = f(x) .\]
  Namely, the distribution $V$ associated with $v(x) = \frac{1}{2}\sin|x|$ satisfies 
  \[\left( \frac{d^2}{dx^2}+ I  \right) \left( V\ast f \right) = \left( V'' + V \right)\ast f = \delta_0 \ast f = f .\]
  $V$ is in a way the fundamental solution of $\Delta + I = f$. 

\end{example}

We wish now to better characterize the dual of $\DD' (\Omega )$. 
\begin{definition}
  The topology on $\DD'(\Omega)$ uses seminorms 
  \[\rho(f)(T) = |T(f)|, \qquad f\in \DD(\Omega).\]
  \cg Remark: this is nothing other than the topology of pointwise convergence! \cbk  
\end{definition}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\begin{theorem}
  If $L:\DD\left( \Omega \right) \to \left( \Omega    \right)$ is linear and continuous, then so is $L' : \DD'(\Omega) \to \DD'(\Omega)$, and $L'T = T\circ L$. 
\end{theorem}
\end{tcolorbox}

\begin{proof}
  Exercise\dots
\end{proof}

\begin{corollary}
  In $\DD'(\Omega)$, if $T_n \to T$ in norm, then $D^\alpha T_n \to D^\alpha T$. This is easy to show. Indeed, consider $D^\alpha T_n f$. By our rules, we just move the derivative on $f$ and get $D^\alpha T_n f = \left( -1 \right)^\alpha T_n D^\alpha f$ which does converge to $ \left( -1 \right)^\alpha T D^\alpha f$ since $T_n \to T$ by assumption. 
\end{corollary}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\begin{theorem}
  $\DD'(\Omega) $ is complete. In particular, if $T_1, T_2, \dots \in \DD'(\Omega)$ and 
  \[Tf = \lim_n T_n f\] exists for every $f \in \DD(\Omega)$, then this defines a distribution $T \in \DD'(\Omega)$. 
\end{theorem}
\end{tcolorbox}

\begin{proof}
  Let $n \mapsto T_n$ be Cauchy in $\DD'(\Omega)$. \cg Recall that by definition, this means that $|\left( T_n - T_m \right)f | \to 0$ for every $f \in \DD$. \cbk Then, the sequence $T_n f$ is a Cauchy sequence in $\R$ or $\C$ for every $f$. By completeness of the field then, the limit exists. We claim that this limiting process defines a new distribution. We need to verify that it is linear and continuous. Clearly, as a limit of linear operators, $T$ is linear. For continuity, we need to establish a bound. To do so, we recall that we need to establish the bound on some $\DD_K$. Let $K \subset \Omega$ be compact. Consider the restriction $T|_{\DD_K}$. Now, we do a familiar style of proof using Uniform Boundedness. First of all, we know that $\DD_K$ is metrizable and complete. By Baire, then $\DD_K$ is of second category. Additionally, we can write $\DD_K = \bigcup_mS_m$, where $S_m = \left\{ f \in \DD_K : \|T_nf\|\leq m \right\}$ are closed. Now from Baire, we conclude that at least one of the $S_m$ contains an interior point. Let us focus on that one now. If $S_m$ contains an interior point, then it includes a set $\left\{ f_0 \in \DD_K : \|f_0 - g\| < r \right\}$. 
  For convenience, let $\|f\|_N = 1$. \cg We could take any value, just a convenient choice.  We will then use linearity to show the result holds for $f$ with arbitrary finite norm. \cbk Then, $g + \frac{r}{2}f \in S_m$. \cg since $\|\frac{r}{2}f\| < r$. \cbk So, 
  \begin{align*}
    \left| {{T_n}\left( f \right)} \right| &= \frac{2}{r}\left| {{T_n}\left( {\frac{r}{2}f + g - g} \right)} \right| \hfill \\[.2cm]
     &\leqslant \frac{2}{r}\left| {{T_n}\left( {g + \frac{r}{2}f} \right)} \right| + \frac{2}{r}\left| {{T_n}\left( g \right)} \right| \hfill \\[.2cm]
     &\leqslant \frac{2}{r}m + \frac{2}{r}m \hfill \\[.2cm]
     &= \frac{{4m}}{r}{\left\| f \right\|_N} 
  \end{align*}
  So $|T(f)| \leq 4m/r \|f\|_N$.
\end{proof}
\begin{ex}
  Let $\Omega=\R$. Consider the sequence of distributions $H_m \in \DD'\left( \R \right)$ that are associated with the functions $h_m(x) = m^2\sin(mx)$. Claim that distributions $H_m \to 0$. To check, just compute $H_m f$ for $f \in \DD$:
  \begin{align*}
    {H_m}f &= \int\limits_\mathbb{R} {{m^2}\sin \left( {mx} \right)f\left( x \right)dx}  \hfill \\
     &=  - \int\limits_\mathbb{R} {m\cos \left( {mx} \right)f'\left( x \right)dx}  \hfill \\
     &= \int\limits_\mathbb{R} {\sin \left( {mx} \right)f''\left( x \right)dx}  \hfill \\
     &=  - \frac{1}{m}\int\limits_\mathbb{R} {\cos \left( {mx} \right)f'''\left( x \right)dx}  \to 0,\qquad \text{as}\qquad m \to 0.
  \end{align*}
\end{ex}
\begin{ex}
  Let $\Omega=\R^n$. Consider the sequence of distributions $H_k \in \DD'\left( \Omega \right)$ associated with the functions $h_k(x) = k^n h(kx)$ where $h \in L^1$ and $\int h = 1$. Claim that $H_k \to \delta_0$ in $\DD'(\Omega)$. 
  Same situation, although this time with a clever change of variables. 
  \[{H_k}f = \int\limits_{{\mathbb{R}^n}} {{k^n}h\left( {kx} \right)f\left( x \right)dx} \]
  Let now $\xi = kx$ so $d \xi = k^ndx$ \cg careful change of variables in $n$ dimensions! \cbk Then \cred something went wrong here... check integration by parts, the end result is I want $H_kf\to f(0)$ as $k \to \infty$. \cbk 
\end{ex}

\subsection{Fundamental Solutions}
In this section, we discuss so called \textit{fundamental} solutions to differential equations. What do we mean by fundamental in this case? We mean loosely that given a differential equation $Lu = f$, we may solve it for any right hand side $f$ by just knowing the fundamental solution. 

\begin{note}
  An integral operator $\hat W$, associated with an integral kernel function $w(x, y)$ is of the form 
  \[\left( {\hat Wh} \right)\left( x \right) = \int {w\left( {x,y} \right)h\left( y \right)dy} .\] 
  We can also regard such operators as distributions in the following sense:
  \[\left( {\hat Wh} \right)\left( f \right) = \int {\int {w\left( {x,y} \right)h\left( y \right)f\left( x \right)dydx} } = W\left( f\times h \right).\]
\end{note}
For a distribution $W\in \DD'\left( \Omega\times \Omega \right)$, we associate with it a linear operator $\hat W : \DD\left( \Omega \right) \to \DD'\left( \Omega \right)$ which is defined by setting
\[\left( \hat W h  \right)  f = W\left( f\times h \right)\]
\cred clarify some understanding here...not exactly sure what is going on...\cbk 

\begin{ex} Consider 
  $J\in \DD'(\Omega\times \Omega)$, defined by 
  \[J(\phi) = \int \phi(x, x)dx.\]
  Then, the associated linear operator is $\hat J: \DD(\Omega) \to \DD'(\Omega)$ is given by 
  our identity, 
  \[\left( {\hat Jh} \right)\left( f \right) = J\left( {f \times h} \right) = \int {f\left( x \right)h\left( x \right)dx}  = H\left( f \right)\]
  \cg Notice the above expression is not actually the linear operator itself, but rather its value with a particular $f \in \DD$. Recall that the associated linear operator $\hat J: \DD(\Omega) \to \DD'(\Omega)$. \cbk i.e., $\hat Jh = H$. In this case, $\hat J$ associates every $h \in \DD(\Omega)$ with its natural distribution $H \in \DD'(\Omega)$, acting like somewhat of an inclusion map. 
  \cblu This example helps to clear things up. We start with a distribution defined on $\Omega\times \Omega$. In this case, that is the distribution $J$ which is a continuous linear functional defined on test functions on $\Omega\times \Omega$. Now, we associate with $J$ the linear operator $\hat J$. Now $\hat J$ takes as argument a test function $h\in \DD(\Omega)$ and returns for you a distribution on $\Omega$. Its value at a particular $f$ is the identity we have. \cbk

\end{ex}
Consider now an $N^{\text{th}}$ order differential operator 
\[L = \sum_{|\alpha|\leq N} g_\alpha D^\alpha, \qquad g_\alpha \in C^\infty(\Omega).\]
\cg Notice that at this point, $g_\alpha$ need not be constant, merely bounded. \cbk 
\begin{definition}
  We say that $W \in \DD'\left( \Omega\times \Omega \right)$ is a fundamental solution of $L$ iff $L\hat W = \hat J$. 
\end{definition}
Then, $\hat W$ is the inverse of $L$. Namely, the (weak, or distributional) solution of the equation $Lf=h$ is $F = \hat W h$, since 
\[LF = L\hat W h = \hat J h = H.\]
\begin{example}
  As seen earlier, the distribution $W\in \DD'\left( \Omega\times \Omega \right)$ associated with the function $w\left( x, y \right) = \frac{1}{2}\sin |x-y|$ is a fundamental solution of $L = D^2 + I $. Let us break this down. The distribution $W\in \DD'\left( \Omega\times \Omega \right)$ is 
  \[W\phi  =  {\iint {\frac{1}{2}\sin \left| {x - y} \right|\phi \left( {x,y} \right)dxdy} } ,\qquad \phi  \in \mathcal{D}\left( {\Omega  \times \Omega } \right).\]
  The corresponding linear operator $\hat W : \DD(\Omega) \to \DD' (\Omega)$ is defined by 
  \[\left( {\hat Wh} \right)f = W\left( {f \times h} \right) = {\iint {\frac{1}{2}\sin \left| {x - y} \right|f\left( x \right)h\left( y \right)dxdy} } .\]
  Indeed, $F=\hat Wh$ is a solution to the original problem, since 
  \[LF = L\hat Wh = \left( {{D^2} + I} \right) {\iint {\frac{1}{2}\sin \left| {x - y} \right|h\left( y \right)dxdy} }  = h\left( x \right).\]
  \cg Recall that we actually did show this previously, just without calling it the fundamental solution. \cbk 
\end{example}
The question now is how to find these fundamental solutions? Firstly, notice that the equation $L\hat Wh = \hat J h$ can be written pointwise as 
\[L\hat Wh\left( f \right) = \hat Wh\left( {L'f} \right) = W\left( {L'f \times h} \right) = \hat Jh\left( f \right) = H\left( f \right)\] where $L'f=\sum_{|\alpha|\leq N} D^\alpha\left( g_\alpha f \right)$.
Returning to our example for a moment, we have that 
\[\iint {\left( {{D^2} + I} \right)\left( f \right)\left( x \right)\frac{1}{2}\sin \left| {y - x} \right|h\left( y \right)dydx} = \int {f\left( x \right)h\left( x \right)dx} \]
or equivalently, 
\[\iint {\left( {{D^2} + I} \right)\left( f \right)\left( x \right)\frac{1}{2}\sin \left| {y - x} \right|dydx} = f\left( y \right)\]
\cred not sure if I quite understand here...\cbk 

More generally, if $W$ is associated with a function $w$, then the equation for $w$ is 
\[\int \left( L'f \right)(x) w(x, y) dx = f(y) .\]

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\begin{theorem}
  Let $\Omega=\R$, $N\geq 2$. Let 
  \[L = \sum_{k=0}^N g_k D^k\] with each $g_k$ smooth (i.e. $C^\infty$) and $g_N \neq 0$ for all $ x \in \R$. (This is critical as if $g_N$ vanishes, $L$ is no longer of order $N$)

  Then $L$ admits a fundamental solution $W$ associated with a function $w$ such that $x \mapsto w\left( x, y \right)$ is of class $C^{N-2}$ for every $y$. 
\end{theorem}
\end{tcolorbox}

\begin{proof}
  Sketch. Given any $y \in \R$, let $\psi_y^\pm$ be solutions of $L\psi = 0$ such that 
  \begin{enumerate}[label=(\arabic*)]
    \item $\left( D^k \psi_y^+ \right)(y) = \left( D^k \psi_y^- \right)(y)$ for $k = 0, \dots, N_2$,
    \item $\left( D^{N-1} \psi_y^+ \right)(y) = \left( D^{N-1}\psi_y^-  \right)(y) + \frac{1}{g_N(y)}$. 
  \end{enumerate}
  Note that this is a solution, not the solution. In particular, one can choose $\psi_y^- = 0$. 
  Then by ODE facts, we have a solution\dots
  
  Now let 
  \[w\left( x, y \right) = \left\{ \begin{matrix}
    {\psi_y^+}&{x \leq y } \\[.2cm] 
    {\psi_y^-} & {x > y} 
  \end{matrix}
  \right. .\]
  Then, the associated distribution $W$ satisfies the original equation \cred (Check this!) \cbk 
\end{proof}

Consider now a differential operator with constant coefficients (which can be thought of as  nothing more than a polynomial in $D$)
\[L = \mathcal P(D) = \sum_{|\alpha|\leq N} c_\alpha D^\alpha .\]

\begin{definition}
  $V\in \DD' (\Omega)$ is a fundamental solution of $\mathcal P(D)$ iff 
  \[\mathcal P(D) V = \delta_0.\]
\end{definition}

\begin{lemma}
  If $V$ satisfies $\mathcal P(D) V = \delta_0$, then $f = V\times h$ satisfies $Lf = h$. 
\end{lemma}
\begin{proof}
  \[ Lf(x)  = L\left( V\times h \right) (x) = \mathcal P(D)\left( V\times h \right) (x) = \left( \mathcal P(D) V \right) \times h (x) = \left( \delta_0 \times h \right)(x) = h(x).\] 
\end{proof}
\begin{note}
  This corresponds to $\hat Wh = V\times h$. If $V$ is associated with a function $v$, then $W$ will be associated with a $w(x, y) = v(x-y)$. 
\end{note}
\begin{ex}
  The Laplacian in 3D admits the following general solution: 
  \[Vf = \lim_{\varepsilon \to 0 } \int\limits_{|x|\leq \varepsilon} v(x)f(x)dx, \qquad v(x) = -\frac{1}{4\pi \|x\|}.\]
  So $\Delta V = \delta$. 
\end{ex}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\begin{theorem}[Malgrange-Ehrenpreis]
  Every $L$ with constant coefficeints admits a fundamental solution $V \in \DD'\left( \R^n \right)$. 
\end{theorem}
\end{tcolorbox}



\subsection{Tempered Distributions}
We concern ourselves now with the space of \textit{rapidly decreasing} functions, denoted $\mathcal S \left( \R^n \right)$. By rapidly decreasing, we mean that such functions decay faster than any polynomial. Notice immediately that $\DD$ is included in $\mathcal S$. Equip $\mathcal S$ with the seminorms 
\[\|f\|_n = \sup_{x \in \R^n} \sup_{|\alpha| + |\beta| \leq N} \left\vert x^\beta\left( D^\alpha f \right)(x) \right\vert.\]
Notice the following properties: 
\begin{itemize}
  \item The sequence of seminorms is increasing. That is, $\|f\|_0 \leq \|f\|_1 \leq \|f\|_2\leq \dots$.
  \item $\mathcal S\left( \R^n \right)$ is metrizable. 
  \item $f \in \mathcal S$ implies $x^\beta D^\alpha f \in \mathcal S$, and 
  \[{\left\| {{x^\beta }\left( {{D^\alpha }f} \right)} \right\|_N} \leqslant C{\left\| f \right\|_{N + \left| \alpha  \right| + \left| \beta  \right|}}.\]
\end{itemize}
As for $\DD_K$ and $\DD$, we define now $\mathcal S' \left( \R^n \right)$ as the space of bounded linear functionals. All in a similar way, we have the following results: 
\begin{itemize}
  \item $\mathcal S$ is complete. 
  \item The topology on $\mathcal S'$ is given by the topology of pointwise convergence, i.e. seminorms $\rho_f (T) = |Tf|$ for $f \in \mathcal S$. 
  \item $\mathcal S'$ is complete. 
  \item If $L : \mathcal S \to \mathcal S$ is continuous and linear, then $L' : \mathcal S' \to \mathcal S'$ is also, and $L'T = T\circ L$. 
  \item $\mathcal S'\left( \R^n \right) \subset\DD' \left( \R^n \right)$. 
\end{itemize}
\begin{ex}
  We know that for a function $h \in L^1_{loc}$ we can associate a distribution $H \in \DD'$. On its own, however, this is not good enough for $\mathcal S$. However, if on the other hand, 
  \[\int \left( 1+|x| \right)^{-N} |h(x)| dx < \infty\] for some $N$, then $H \in \mathcal S'$. Notice in particular that 
  \[Hf = \int {h\left( x \right)f\left( x \right)dx}  = \int {\underbrace {{{\left( {1 + \left| x \right|} \right)}^{ - N}}h\left( x \right)}_{ < \infty }\underbrace {{{\left( {1 + \left| x \right|} \right)}^N}f\left( x \right)}_{ \leqslant C{{\left\| f \right\|}_N}}dx}. \]
\end{ex}
\begin{ex}
  $f \in L^p$.
\end{ex}
\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\begin{theorem}
  Here, we use the following notation: $\left( {{M^\alpha }f} \right)\left( x \right) = {x^\alpha }f\left( x \right)$.
  
  If $T\in \ScS'$ and $f \in \ScS$, then $T\ast f$ is smooth and polynomially bounded. 
\end{theorem}
\end{tcolorbox}


\subsection{Fourier Transform}

Recall the following facts about the Fourier Transform as we have used before: 
\[\left( {\mathcal{F}f} \right)\left( k \right) = \tilde f\left( k \right) = {\left( {2\pi } \right)^{ - n/2}}\int {e^{ - ik \cdot x}}f\left( x \right)dx\]
for $f \in L^1$, say. We also have the nice property that under the Fourier Transform, convolution turns into multiplication, i.e. 
\[\mathcal{F}\left( {f * g} \right) = {\left( {2\pi } \right)^{n/2}}\left( {\mathcal{F}f} \right)\left( {\mathcal{F}g} \right).\]


\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
  \begin{theorem}
    For $f \in \ScS$, 
    \[\mathcal{F}\left( {{M^\alpha }f} \right) = {\left( {iD} \right)^\alpha }\mathcal{F}f.\]
    \[\mathcal{F}\left( {{D^\alpha }f} \right) = {\left( {iM} \right)^\alpha }\mathcal{F}f.\]
    \[\mathcal{F}^2f = \hat f.\]


  \end{theorem}
\end{tcolorbox}

\begin{ex}
  $n = 1$, $f_0 = e^{-\frac{1}{2}|x|^2}$, $\mathcal F f_0 = f_0$. Let $A = M + D$ and $f_n = A^n f_0$. Then $\mathcal F f_n = \pm i^n f_n$. 
\end{ex}

Some more facts: 
\begin{itemize}
  \item $\mathcal F: \ScS \to \ScS$ is continuous. 
  \item The Fourier Transform of a distribution is defined as 
  \[\left( {\mathcal{F}T} \right)\left( f \right) = T\left( {\mathcal{F}f} \right), \qquad f \in \ScS.\]
\end{itemize}

\begin{ex}
  The Fourier Transform of the delta distribution is the distribution associated with the constant function $1$, and vice versa. That is, $\tilde \delta = 1$, and $\tilde 1 = \delta$. 

  \[\tilde \delta f = \delta \left( {\tilde f} \right) = \tilde f\left( 0 \right) = {\left( {2\pi } \right)^{ - n/2}}\int 1 f\left( x \right)dx = {\left( {2\pi } \right)^{ - n/2}} 1\left( f \right)\]

  \[\tilde 1f = 1\left( {\tilde f} \right) = \int {\tilde f} \left( k \right)dk = {\left( {2\pi } \right)^{n/2}}\left( {{\mathcal{F}^{ - 1}}\tilde f} \right)\left( 0 \right) = {\left( {2\pi } \right)^{n/2}}\delta f\]


\end{ex}
Now to solve $\mathcal P(D) T = \delta$, one way to do it is with the Fourier Transform. Under Fourier, the differential equation becomes 
\[\mathcal P \left( iM \right) \tilde T = \left( 2\pi  \right)^{-n/2} 1.\]
If $\mathcal P(ik) \neq 0$ for every $k\in \R^n$, then just divide and get the solution in the Fourier domain: 
\[\tilde T(k) = \frac{\left( 2\pi \right)^{-n/2}}{\mathcal P\left( ik \right)}.\]


\end{document}
 