\documentclass[letterpaper,twoside,12pt]{article}
\usepackage{a4wide,graphicx,fancyhdr,clrscode,tabularx,amsmath,amssymb,amsfonts,color,enumitem, bm, bbm, array, textcomp, subcaption, color, listings, chemformula, tcolorbox, setspace, xcolor, hyperref}
\usepackage{amsthm}		%theorem style 
%\usepackage{geometry}  %Adjust margins if needed. 
%\geometry{margin=1.25in} 
%\usepackage{mathptmx}      %SET MATH TYPE FONT TO TIMES NEW ROMAN
%These lines make the theorem NAME BOLD
\newtheoremstyle{mystyle}%                % Name
  {}%                                     % Space above
  {}%                                     % Space below
  {\itshape}%                                     % Body font
  {}%                                     % Indent amount
  {\bfseries}%                            % Theorem head font
  {.}%                                    % Punctuation after theorem head
  { }%                                    % Space after theorem head, ' ', or \newline
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}%                                     % Theorem head spec (can be left empty, meaning `normal')
\theoremstyle{mystyle}
\newtheorem{theorem}{Theorem}[section]
%end of making theorem name bold. 
\newtheorem*{thm}{Theorem}		%Theorem no number. 
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{lemm}{Lemma}
\newtheorem{slem}{Sub-Lemma}
\newtheorem{prop}{Proposition}[section]
\newtheorem*{propp}{Proposition}
\newtheorem*{ex}{Example}
\newtheorem*{example}{Example}
\newtheorem{notee}{Note}[section]
\newtheorem*{note}{Note}
\usepackage[super]{nth}
\usepackage[makeroom]{cancel}

%----------------------- Macros and Definitions --------------------------
\setlength{\fboxsep}{2.5\fboxsep}
%Sets boxlength size

\setlength\headheight{15pt}
\addtolength\topmargin{-25pt}
\addtolength\footskip{0pt}

\fancypagestyle{plain}{%
\fancyhf{}
\fancyhead[LO,RE]{\sffamily UT Austin}
\fancyhead[RO,LE]{\sffamily CSE 386D}
\fancyfoot[LO,RE]{\sffamily Oden Institute}
\fancyfoot[RO,LE]{\sffamily\bfseries\thepage} 
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[RO,LE]{\sffamily CSE 386D}
\fancyhead[LO,RE]{\sffamily UT Austin}
\fancyfoot[LO,RE]{\sffamily Oden Institute}
\fancyfoot[RO,LE]{\sffamily\bfseries\thepage}
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{0pt}
\newcommand{\R}{{\mathbb R}}
\newcommand{\N}{{\mathbb N}}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\Q}{{\mathbb Q}}
\newcommand{\C}{{\mathbb C}}
\newcommand{\SL}{{\mathcal{L}}}
\newcommand{\DD}{\mathcal D}
\newcommand{\ScS}{\mathcal S}
\newcommand{\Om}{\Omega}
\DeclareMathOperator*{\slim}{s-lim}
\newcommand{\cg}{\color{gray}}
\newcommand{\cbk}{\color{black}}
\newcommand{\cred}{\color{red}}
\newcommand{\cblu}{\color{blue}}
\newcommand{\ve}{\varepsilon}
\newcommand{\inv}{^{-1}}
\usepackage{libertine}            %% For fancy font
\begin{document}
%\fontfamily{ptm}\selectfont     %% TO SELECT THE FONT
\title{\vspace{-2\baselineskip} 
Review
}
\author{Jonathan Zhang}
\date{}
\maketitle

%\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
%  \begin{theorem}
%    Here, we use the following notation: $\left( {{M^\alpha }f} \right)\left( x \right) = {x^\alpha }f\left( x \right)$.
%    
%    If $T\in \ScS'$ and $f \in \ScS$, then $T\ast f$ is smooth and polynomially bounded. 
%  \end{theorem}
%  \end{tcolorbox} 

\section{Differential Calculus in Banach Spaces and the Calculus of Variations}
\subsection{Differentiation}

  Our goal will be to generalize the notion of the 1D derivative, in the context of results like the Mean Value Theorem and Taylor's Theorem. Let us start with what we know: the standard 1D definition of the derivative is
  \begin{equation}
    f'(x) = \lim_{h\to 0} \frac{f(x+h) - f(x)}{h},
  \end{equation}
  but it is not clear how $1/h$ will generalize in higher dimensions. Indeed, it makes no sense. We can, however, define a similar concept. 
  \begin{definition}[Little o]
    Let $X, Y$ be NLS's, $f : X \to Y$. If 
    \[\lim_{h \to 0} \frac{\|f(h)\|_Y}{\|h\|_X} = 0,\]
    we say that $f$ is ``little oh'' of $h$ and write 
    \[\|f(h)\|_Y = o\left( \|h\|_X \right).\]
  \end{definition}
  \cg
    Here, we have generalized the notion of $1/h$ to multiple dimensions by looking only at the norm of the increment and of the output. 
  \cbk
  We can now formulate a notion of the derivative.
  \begin{definition}[Fr\'echet Derivative]
    Let $X, Y$ NLS, $U\subset X$ open, $f: U \to Y$. We say that $f$ is Fr\'echet differentiable (or strongly differentiable) at a point $x \in U$ if and only if there exists a bounded linear map $A \in B(X, Y)$ such that the remainder
    \[R(x,h) := f(x+h) - f(x) - Ah\]
    is $o\left( \|h\|_X \right)$. That is, 
    \[\lim_{h\to 0}\frac{\|R(x,h)\|_Y}{\|h\|_X} = 0.\]
  \end{definition}
  If $A$ exists, we call $A$ the Fr\'echet derivative of $f$ at $x$, denoted $Df(x)$. Note that by construction, $A = Df(x)$ is a linear operator on $h$. 
  The definition of the Fr\'echet derivative is consistent with the one dimensional analogue. For $f \in C^1 (\R)$, 
  \begin{equation}
    R\left( {x,h} \right) = f\left( {x + h} \right) - f\left( x \right) - f'\left( x \right)h = \left[ {\frac{{f\left( {x + h} \right) - f\left( x \right)}}{h} - f'\left( x \right)} \right]h,
  \end{equation}
  and 
  \begin{equation}
    \mathop {\lim }\limits_{h \to 0} \frac{{\left| {R\left( {x,h} \right)} \right|}}{{\left| h \right|}} = \mathop {\lim }\limits_{h \to 0} \left[ {\frac{{f\left( {x + h} \right) - f\left( x \right)}}{h} - f'\left( x \right)} \right] = 0.
  \end{equation}
  More generally, the map $Df$ (as contrasted with $Df(x)$) can be regarded as a map $Df: U\times X \ni (x, h) \to Df(x)h \in Y$. 
  \begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
    \begin{prop}
      If $f$ is Fr\'echet differentiable, then $Df(x)$ is unique, and $f$ is continuous at $x$. Moreover, if $g$ is Fr\'echet differentiable, then for every $\alpha, \beta \in \mathbb F$, 
      \[D\left( \alpha f + \beta g \right)(x) = \alpha Df(x) + \beta Dg(x).\]
      (That is to say, the operator $D$ taking functions $f \mapsto Df$ is linear.) 
    \end{prop}
  \end{tcolorbox}
  \begin{proof}
    Suppose that there are two derivatives at $x$, $A, B \in B(X, Y)$. Define the two remainders
    \begin{equation}
      \begin{gathered}
        {R_A}\left( {x,h} \right) = f\left( {x + h} \right) - f\left( x \right) - Ah \hfill \\[.2cm]
        {R_B}\left( {x,h} \right) = f\left( {x + h} \right) - f\left( x \right) - Bh. 
      \end{gathered} 
    \end{equation}
    By assumption, both $R_A(x,h) = o\left( \|h\|_X \right)$ and $R_B(x, h) = o\left( \|h\|_X \right)$. Now we show that in fact, $A = B$. 
    \begin{equation}
      \begin{split}
        {\left\| {A - B} \right\|_{B\left( {X,Y} \right)}} &= \cred \mathop {\sup }\limits_{\left\| h \right\| = \varepsilon }\cbk  \frac{{{{\left\| {Ah - Bh} \right\|}_Y}}}{{{{\left\| h \right\|}_X}}} \hfill \\[.2cm]
        &= \mathop {\sup }\limits_{\left\| h \right\| = \varepsilon } \frac{{{{\left\| {Ah - Bh} \right\|}_Y}}}{{{{\left\| h \right\|}_X}}} \hfill \\[.2cm]
        &= \mathop {\sup }\limits_{\left\| h \right\| = \varepsilon } \frac{{{{\left\| {{R_A}\left( {x,h} \right) - {R_B}\left( {x,h} \right)} \right\|}_Y}}}{{{{\left\| h \right\|}_X}}} \hfill \\[.2cm]
        &\leqslant \mathop {\sup }\limits_{\left\| h \right\| = \varepsilon } \frac{{{{\left\| {{R_A}\left( {x,h} \right)} \right\|}_Y}}}{{{{\left\| h \right\|}_X}}} + \mathop {\sup }\limits_{\left\| h \right\| = \varepsilon } \frac{{{{\left\| {{R_B}\left( {x,h} \right)} \right\|}_Y}}}{{{{\left\| h \right\|}_X}}} .
      \end{split}
    \end{equation}
    \cred Is this the right definition of the norm on $B(X,Y)$? \cbk The RHS can be made small as $\ve \to 0$. Indeed, the argument of the supremum goes to zero as $\|h\| \to 0$ by the $o(\|h\|_X)$ assumption. Thus, $A = B = Df(x)$ is unique. 
    To prove that $f$ is continuous at $x$, we have 
    \begin{equation}
      \begin{split}
          {\left\| {f\left( {x + h} \right) - f\left( x \right)} \right\|_Y}& = {\left\| {Df\left( x \right)h + R\left( {x,h} \right)} \right\|_Y} \hfill \\[.2cm]
           &\leqslant {\left\| {Df\left( x \right)h} \right\|_Y} + {\left\| {R\left( {x,h} \right)} \right\|_Y} \hfill \\[.2cm]
           &\leqslant {\left\| {Df\left( x \right)} \right\|_{B\left( {X,Y} \right)}}{\left\| h \right\|_X} + {\left\| {R\left( {x,h} \right)} \right\|_Y}.
      \end{split}
    \end{equation}
    The RHS tends to $0$ as $h \to 0$ in $X$. \cg Since $Df(x)$ is bounded, the first term on the right tends to zero trivially as $h \to 0$. Since $R(x,h)$ is $o(\|h\|)$, it means that $R(x,h)$ goes to zero faster than $h$ goes to zero. This implies, however, that if $h\to 0$, then also $R(x, h) \to 0$. A somewhat obvious fact. \cbk 
  \end{proof}
  Having differentiability, namely \textit{strong} differentiability, at a point $x$ is more than enough to conclude that $f$ is continuous at $x$. A piecewise linear function may be continuous, but not differentiable at the interface points. Strong differentiability allows us to conclude more: 
  \begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
    \begin{lemma}[Local Lipschitz Property]
      If $f:U \to Y$ is differentiable at $x \in U$, then for every $\ve > 0$, there exists a $\delta = \delta(x, \ve) > 0$ such that for every $h$ with $\|h\|_X \leq \delta$, 
      \[{\left\| {f\left( {x + h} \right) - f\left( x \right)} \right\|_Y} \leqslant \left( {{{\left\| {Df\left( x \right)} \right\|}_{B\left( {X,Y} \right)}} + \varepsilon } \right){\left\| h \right\|_X}.\]
    \end{lemma} 
  \end{tcolorbox}
  \begin{proof}
    We have 
    \begin{equation}
      f\left( {x + h} \right) - f\left( x \right) = Df\left( x \right)h + R\left( {x,h} \right).
    \end{equation}
    Since the remainder is $o(\|h\|)$, then for every $\ve > 0$, there exists a $\delta > 0$ such that 
    \begin{equation}
      \frac{{{{\left\| {R\left( {x,h} \right)} \right\|}_Y}}}{{{{\left\| h \right\|}_X}}} < \ve
    \end{equation}
    whenever $\|h\|_X < \delta$. \cg Definition of convergence of $\|R\| /\|h\| \to 0$. \cbk It follows then that 
    \begin{equation}
      {\left\| {f\left( {x + h} \right) - f\left( x \right)} \right\|_Y} = \left\| {Df\left( x \right)h + R\left( {x,h} \right)} \right\| \leqslant {\left\| {Df\left( x \right)} \right\|_{B\left( {X,Y} \right)}}{\left\| h \right\|_X} + \varepsilon {\left\| h \right\|_X}.
    \end{equation}
  \end{proof}
  Even if $f$ is not strongly differentiable, it may still admit some kind of weaker derivative. The key here is that in the Fr\'echet derivative, we require the remainder goes to zero for all paths that take $\|h\|_X \to 0$. We now consider incrementing only in a specific direction $h$. Start with $f:U \to Y$, fix a direction $h \in X$, and consider the auxillary function 
  \begin{equation}
    g:\R \ni t \to g(t) := f(x + th) \in Y.
  \end{equation}



  \begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
    \begin{prop}
      \[f\text{ is Fr\'echet differentiable at }x \implies f\text{ is Gateaux differentiable at }x. \]
    \end{prop}
  \end{tcolorbox}

  \begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
    \begin{theorem}[Chain Rule]
      Let $X, Y, Z$ be NLS's and $U\subset X$, $V\subset Y$ open, $f : U \to Y$ and $g : V \to Z$. Let $x \in U$ and $y = f(x) \in V$. Suppose that $g$ is Fr\'echet differentiable at $y$ and $f$ is Gateaux (or Fr\'echet) differentiable at $x$. Then the composition $g \circ f$ is Gateaux (or Fr\'echet) differentiable at $x$ with 
      \[D(g\circ f)(x) = Dg(y) \circ Df(x)\]
    \end{theorem}
  \end{tcolorbox}

  \begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
    \begin{prop}[Mean Value Theorem for Curves]
      Let $Y$ be an NLS and $\varphi : [a,b] \to Y$ be continuous, where $a < b$ are real numbers. Suppose $\varphi ' (t)$ exists on $(a,b)$ and $\|\varphi'(t)\|_{B(\R,Y)} \leq M$. Then 
      \[\|\varphi(b) - \varphi(a)\|_Y \leq M(b-a).\]
    \end{prop}
  \end{tcolorbox}
  \begin{proof}
    Use a compactness argument. 
  \end{proof}

  \begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
    \begin{theorem}[Mean Value Theorem]
      Let $X,Y$ be NLS's, $U\subset X$ open. Let $f: U \to Y$ be Fr\'echet differentiable at all $x \in U$, and suppose that the line segment 
      \[\ell = \left\{ tx_2 + (1-t)x_1 : t\in [0,1]\right\} \]
      is contained in $U$. Then 
      \[{\left\| {f\left( {{x_2}} \right) - f\left( {{x_1}} \right)} \right\|_Y} \leqslant \mathop {\sup }\limits_{x \in \ell } {\left\| {Df\left( x \right)} \right\|_{B\left( {X,Y} \right)}}{\left\| {{x_2} - {x_1}} \right\|_X}.\]
    \end{theorem}
  \end{tcolorbox}
  \begin{proof}
    Use the MVT for curves applied to the function $\varphi : [0,1]\to Y$, 
    \[\varphi \left( t \right) = f\left( {\left( {1 - t} \right){x_1} + t{x_2}} \right) = f\left( {{x_1} + t\left( {{x_2} - {x_1}} \right)} \right) = f\left( {\gamma \left( t \right)} \right).\]
  \end{proof}

  \begin{definition}
    Let $X = X_1 \oplus \dots \oplus X_m$. Let $U \subset X$ open, $F:U\to Y$, $Y$ NLS. Let $x = (x_1, \dots, x_m)\in U$ and fix an integer $k \in [1,m]$. For $z$ near $x_k$ (k-th component) in space $X_k$, the point $(x_1, \dots, x_{k-1}, z, x_{k+1}, \dots, x_m) \in U$ since $U$ is open. Define now 
    \[f_k(z) = F(x_1, \dots, x_{k-1}, z, x_{k+1}, \dots, x_m).\]
    Then $f_k$ maps an open set in $X_k$ to $Y$. If $f_k$ has a Fr\'echet derivative at $z=x_k$, then we say that the original $F$ has a k-th partial derivative at $x$ and define $D_kF(x) := Df_k (x_k)$. Notice that $D_kF(x) = Df_k (x_k) \in B(X_k, Y)$. 
  \end{definition}

  \begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
    \begin{prop}
      Let $X = X_1 \oplus \dots \oplus X_m$ be the direct sum of NLS's, $U\subset X$ open, and $F : U \to Y$, $Y$ an NLS. Suppose that all partial derivatives $D_jF(x)$ exist for $x \in U$ and $j \in [1,m]$, and that these maps are continuous as a function of $x$ at some $x_0 \in U$. Then, $F$ is Fr\'echet differentiable at $x_0$ and for an increment $h = (h_1, \dots, h_m) \in X$, 
      \[DF(x_0)h = \sum_{j = 1}^{m}D_jF(x_0)h_j.\]
    \end{prop}
  \end{tcolorbox}








\subsection{Fixed Points and Contractive Maps}

  \begin{definition}[Contraction Map, Fixed Point]
    Let $(X,d)$ be a metric space, $G:X\to X$. We say that the mapping $G$ is a contraction if and only if there exists a \(theta\), \(0 \leq \theta < 1\) such that 
    \[d\left( G(x), G(y) \right) \leq \theta d(x, y)\]
    for every $x, y \in X$. We say that a point $x \in X$ is a fixed point of the mapping $G$ if and only if $x = G(x)$. 
  \end{definition}
  The definition of a contraction immediately implies that every contraction is Lipschitz continuous with Lipschitz constant strictly smaller than 1. 

  \begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
    \begin{theorem}[Banach Contraction Mapping Theorem]
      Let \((X,d)\) be a complete metric space and \(G\) a contraction on \(X\). Then there is a unique fixed point of \(G\) in $X$. 
    \end{theorem}
  \end{tcolorbox}
  \begin{proof}
    Uniqueness: go by contradition, then you would get \(x=y\). 
    Existence: Fix $x_0 \in X$. Consider the sequence \(x_n = G(x_{n-1})\). Claim: $x_n$ is a Cauchy sequence. Observe that 
    \[d(x_1, x_2) = d(G(x_0), G(x_1))\leq \theta d(x_0, x_1)\]
    and proceed by induction. Then you find out 
    \[d(x_n, x_{n+1}) \leq \theta^n d(x_0, x_1).\] 
    Then for $n \geq 0$, $m > n$, 
    \begin{align*}
      d(x_n, x_m) &\leq d(x_n, x_{n+1}) + d(x_{n+1}, x_{n+2}) + \dots + d(x_{m-1}, x_m)\\
      &\leq \left( \theta^n  + \dots + \theta^{m-1} \right) d\left( x_0, x_1 \right)\\
      &=\theta^n \left( 1 + \theta + \dots + \theta^{m-n-1} \right)d(x_0, x_1)\\
      &\leq \frac{\theta^n}{1-\theta}d(x_0, x_1).
    \end{align*}
    Then since $X$ is complete, the Cauchy sequence \(x_n\) converges to the limit, the fixed point. 
  \end{proof}
  \begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
    \begin{corollary}[Fixed Point Iteration]
      Let the assumptions of Theorem 1.4 hold. Suppose that \(G\) has a contraction constant \(\theta\), and let $x_0 \in X$. Then, if the sequence $x_n$ is defined successively by \(x_{n+1} = G(x_n)\), then \(x_n \to x\), where $x$ is the unique fixed point of \(G\) in \(X\). Moreover, 
      \[d(x_n, x) \leq \frac{\theta^n}{1-\theta}d(x_0, x_1).\]
    \end{corollary}
  \end{tcolorbox}
  Usually, the metric space \(X\) will be the space of continuous functions. For nonlinear equations, you may have to restrict yourself to a ball, $B_r(0) \subset X = C^0([0,T])$, or something similar. Once you take the closed ball, you have still a metric space, so everything else still works. 
  \begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
    \begin{corollary}
      Let \(X\) be a Banach space and \(f: X \to X\) a differentiable mapping. Suppose that $\|Df(x)\|_{B(X,X)} \leq \kappa < 1$ for $x \in \overline{B_R(0)}$. If there is an $x_0 \in B_R(0)$ such that \(\overline{B_r(x_0)}\subset B_R(0)\) for some $r \geq \|f(x_0) - x_0\|/(1-\kappa)$, then \(f\) has a unique fixed point in \(\overline{B_r(x_0)}\).  
    \end{corollary}
  \end{tcolorbox}
  That is to say, a map \(f\) which is \textit{locally contractive}, and for which we can find a point that is not too far moved by \(f\), has a fixed point which may be obtained by the sequence \(x_{n+1} = f(x_n)\). 








\subsection{Nonlinear Equations}
  We have seen that the basic tool for proving the existence and uniqueness of nonlinear equations is the contraction mapping theorem. We consider now the more general problem of solving \(f(x) = y\), where $f:X\to Y$ and \(X, Y\) are Banach spaces. 
  Solving \(f(x) = y\) is equivalent to solving for the fixed point of the operator 
  \[G(x) = x - T_x(f(x) - y).\]
  where \(T_x : Y \to X\) vanishes only at $0$.
  \cg \(x = G(x) \implies 0 = T_x(f(x) - y)\implies f(x) =y. \)\cbk 
  If \(T\) is independent of \(x\), then we also have 
  \[DG(x) = I - T\circ Df(x).\]
  If $\|DG(x)\|\leq \kappa < 1$, that is, if we can select such a $T$ to make the operator norm less than one, then we can apply the result of Corollary 1.4.2 in an attempt to find a fixed point. The result is 
  \begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
    \begin{theorem}[Simplified Newton Method]
      Let \(X, Y\) Banach, \(f:X\to Y\) a differentiable mapping. Suppose that \(A=Df(x_0)\) has a bounded inverse, and that 
      \[\|I-A\inv Df(x)\| \leq \kappa <1\]
      for every $x \in B_r(x_0)$ for some $r>0$. Let 
      \[\delta = \frac{(1-\kappa)r}{\|A\inv\|_{B(Y,x)}}.\]
      Then the equation $f(x) = y$ has a uniue solution \(x\in B_r(x_0)\)whenever \(y\in B_{\delta}(f(x_0))\).
    \end{theorem}
  \end{tcolorbox}
  \begin{proof}
    Let \(y\in B_{\delta}(f(x_0))\) be given, define a mapping 
    \[g_y:X \to X, \qquad x \longmapsto x - A\inv(f(x)-y).\]
    Note that $f(x) = y$ if and only if $g_y(x) = x$. \cg Same argument as previous. If $A$ is invertible, then $A\inv$ vanishes only at zero. \cbk Notice also that since $A\inv$ is independent of \(x\), we have 
    \[Dg_y(x) = I - A\inv Df(x).\]
    By assumption, we have that $\|Dg_y(x)\| = \|I - A\inv Df(x)\| \leq \kappa$ for $x \in B_r(x_0)$, so \(g_y\) is a contraction on \(\overline{B_r(x_0)}\). What remains to show is that $g_y$ takes $B_r(x_0)$ into itself. But with our constructions of $y, \delta$, \cred revisit this...f(x0) not f(x)! 
    \begin{align*}
      \|g_y(x) - x_0\| &\leq \|g_y(x) - g_y(x_0)\| + \|g_y(x_0) - x_0\| \\
      &\leq \dots \\
      &\leq r.
    \end{align*} \cbk 
    The contraction map theorem now implies the existence of a unique solution in the ball. 
  \end{proof}
  \begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
    \begin{theorem}[Newton-Kantorovich Method]
      Let \(X, Y\) be Banach spaces, \(f:X\to Y\) a differentiable mapping. Assume that there is an \(x_0 \in X\) and an \(r>0\) such that 
      \begin{enumerate}
        \item \(A=Df(x_0)\) has a bounded inverse,
        \item \(\|Df(x_1) - Df(x_2)\| \leq \kappa \|x_1-x_2\|\)
      \end{enumerate}
      for every \(x_1, x_2 \in B_r(x_0)\). Let \(y\in Y\), and set 
      \[\ve = \|A\inv (f(x_0)-y)\|_X.\]

    \end{theorem}
  \end{tcolorbox}

  \begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
    \begin{theorem}[Inverse Function Theorem I]
      
    \end{theorem}
  \end{tcolorbox}

  \begin{definition}[Diffeomorphism]
    
  \end{definition}
  
  \begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
    \begin{theorem}[Inverse Function Theorem II]
      
    \end{theorem}
  \end{tcolorbox}

  \begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
    \begin{lemma}
      
    \end{lemma}
  \end{tcolorbox}

  \begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
    \begin{theorem}[Implicit Function Theorem]
      
    \end{theorem}
  \end{tcolorbox}
  \begin{corollary}
    
  \end{corollary}




\subsection{Higher Derivatives}





\subsection{Extrema}





\subsection{Euler-Lagrange Equations}





\subsection{Constrained Extrema and Lagrange Multipliers}





\end{document}
 